{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse DRF Files to get Relevant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_master_df(path, num_races):\n",
    "    '''\n",
    "        Generate the master dataframe from which we will create our training/testing data\n",
    "        \n",
    "        Args:\n",
    "            path (string): Path to directory containing DRF files to parse\n",
    "            num_races (int): Number of races to use in each sequence (how many races back\n",
    "                             are we looking?)\n",
    "        \n",
    "        Returns: Dataframe containing all data from each DRF concatted together\n",
    "    '''\n",
    "    # Cap num_races\n",
    "    num_races = min(num_races, 9) # Only have max of 9 prev race's data\n",
    "    \n",
    "    # Get all DRF files in data directory\n",
    "    filenames = [path+file for file in os.listdir(path) if file.endswith(\".DRF\")]\n",
    "    \n",
    "    # Iterate through each file and concat data to master df\n",
    "    master_df = None\n",
    "    for ii, file in enumerate(filenames): \n",
    "        if ii == 0:\n",
    "            # First pass through just create master df\n",
    "            df = pd.read_csv(file, header=None)\n",
    "            master_df = slice_df(df, num_races)\n",
    "        else:\n",
    "            # All other passes, append sliced dataframe to master\n",
    "            df = pd.read_csv(file, header=None)\n",
    "            df = slice_df(df, num_races)\n",
    "            master_df = master_df.append(df, ignore_index=True)\n",
    "            \n",
    "    # Drop all rows containing NaN values (these horses didn't have enough prev races)\n",
    "    return master_df.dropna().reset_index().drop(['index'], axis=1)\n",
    "\n",
    "def slice_df(df, num_races=3):\n",
    "    # Define columns to grab\n",
    "    column_ids = OrderedDict({\n",
    "        'horse_age': (46,47),\n",
    "        'days_since_prev_race': (266, 266+num_races),\n",
    "        'distance': (316, 316+num_races),\n",
    "        'num_entrants': (346, 346+num_races),\n",
    "        'post_position': (356, 356+num_races),\n",
    "        'weight': (506, 506+num_races),\n",
    "        'label': (1036, 1036+num_races) # Finish time\n",
    "    })\n",
    "\n",
    "    # Select all of our column ranges\n",
    "    rng = []\n",
    "    col_names = []\n",
    "    for k,v in column_ids.items():\n",
    "        # Append range to rng -- special case for single field\n",
    "        if v[1] - v[0] == 1:\n",
    "            for i in range(num_races):\n",
    "                rng += [v[0]]\n",
    "                col_names.append('{}_{}'.format(k, i))\n",
    "        else:\n",
    "            # Handle column ranges\n",
    "            rng += range(v[0],v[1])\n",
    "            for ii in range(v[0], v[1]):\n",
    "                col_names.append('{}_{}'.format(k, ii-v[0]))\n",
    "\n",
    "    # Slice df on columns\n",
    "    ret = df.loc[:, rng]\n",
    "    ret.columns = col_names\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_entrants_0</th>\n",
       "      <th>num_entrants_1</th>\n",
       "      <th>num_entrants_2</th>\n",
       "      <th>days_since_prev_race_0</th>\n",
       "      <th>days_since_prev_race_1</th>\n",
       "      <th>days_since_prev_race_2</th>\n",
       "      <th>weight_0</th>\n",
       "      <th>weight_1</th>\n",
       "      <th>weight_2</th>\n",
       "      <th>distance_0</th>\n",
       "      <th>...</th>\n",
       "      <th>distance_2</th>\n",
       "      <th>post_position_0</th>\n",
       "      <th>post_position_1</th>\n",
       "      <th>post_position_2</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>horse_age_0</th>\n",
       "      <th>horse_age_1</th>\n",
       "      <th>horse_age_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>75.17</td>\n",
       "      <td>73.40</td>\n",
       "      <td>74.41</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>75.20</td>\n",
       "      <td>73.53</td>\n",
       "      <td>80.98</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1830.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>104.90</td>\n",
       "      <td>75.43</td>\n",
       "      <td>108.81</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>103.83</td>\n",
       "      <td>101.39</td>\n",
       "      <td>109.50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>96.75</td>\n",
       "      <td>106.14</td>\n",
       "      <td>100.58</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_entrants_0  num_entrants_1  num_entrants_2  days_since_prev_race_0  \\\n",
       "0             9.0             7.0            10.0                    26.0   \n",
       "1             8.0             8.0             8.0                    14.0   \n",
       "2             7.0             9.0             8.0                   147.0   \n",
       "3             5.0             5.0             7.0                    23.0   \n",
       "4             7.0            12.0             8.0                    63.0   \n",
       "\n",
       "   days_since_prev_race_1  days_since_prev_race_2  weight_0  weight_1  \\\n",
       "0                    80.0                    11.0     121.0     121.0   \n",
       "1                    46.0                    29.0     124.0     119.0   \n",
       "2                    17.0                    13.0     123.0     121.0   \n",
       "3                    89.0                    14.0     121.0     121.0   \n",
       "4                    11.0                    20.0     123.0     123.0   \n",
       "\n",
       "   weight_2  distance_0     ...       distance_2  post_position_0  \\\n",
       "0     123.0      1320.0     ...           1320.0              3.0   \n",
       "1     124.0      1320.0     ...           1430.0              3.0   \n",
       "2     120.0      1760.0     ...           1830.0              5.0   \n",
       "3     122.0      1760.0     ...           1760.0              4.0   \n",
       "4     118.0      1760.0     ...           1760.0              7.0   \n",
       "\n",
       "   post_position_1  post_position_2  label_0  label_1  label_2  horse_age_0  \\\n",
       "0              1.0              9.0    75.17    73.40    74.41            2   \n",
       "1              5.0              8.0    75.20    73.53    80.98            4   \n",
       "2              1.0              5.0   104.90    75.43   108.81            4   \n",
       "3              2.0              6.0   103.83   101.39   109.50            4   \n",
       "4              5.0              2.0    96.75   106.14   100.58            2   \n",
       "\n",
       "   horse_age_1  horse_age_2  \n",
       "0            2            2  \n",
       "1            4            4  \n",
       "2            4            4  \n",
       "3            4            4  \n",
       "4            2            2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days_in_sequence = 3\n",
    "master_df = generate_master_df('./input_files/', days_in_sequence)\n",
    "master_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloader\n",
    "Create a generator that can parse through the master dataframe, and create batches of training data. These batches will have the shape (days_in_sequence, batch_size, input_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Section off data by race -- list of tuples (race_num, data)\n",
    "race_data = []\n",
    "for ii in range(days_in_sequence):\n",
    "    # Match all collumns for this race\n",
    "    pattern = re.compile('.*_{}'.format(ii))\n",
    "    cols = [pattern.match(col).string for col in master_df.columns if pattern.match(col) != None]\n",
    "    # Get data from these columns\n",
    "    data = master_df.loc[:, cols]\n",
    "    # Rename columns\n",
    "    cols = [col[:-2] for col in cols]\n",
    "    data.columns = cols\n",
    "    # Append to race data\n",
    "    race_data.append((ii, data)) \n",
    "    \n",
    "# Break race_data into input_data and label_data\n",
    "input_data = []\n",
    "labels = []\n",
    "for race_tup in race_data:\n",
    "    input_data.append(race_tup[1].drop(['label'], axis=1).values)\n",
    "    labels.append(race_tup[1]['label'].values)\n",
    "    \n",
    "# Want data to go in reverse order (oldest races first)\n",
    "input_data.reverse()\n",
    "labels.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(data, labels, days_in_sequence=3, batch_size=10, input_features=6):\n",
    "    # Truncate data to ensure only full batches\n",
    "    num_horses = len(data[0])\n",
    "    cutoff = (num_horses//batch_size)*batch_size\n",
    "    trunc_data = [race[:cutoff] for race in data]\n",
    "    trunc_labels = [race[:cutoff] for race in labels]\n",
    "    \n",
    "    # Create our batches\n",
    "    for ii in range(0, cutoff, batch_size):\n",
    "        # Get data for this batch\n",
    "        batch_data = [race[ii:ii+batch_size] for race in trunc_data]\n",
    "        batch_labels = [race[ii: ii+batch_size] for race in trunc_labels]\n",
    "        \n",
    "        # Create batch tensor of correct size -- days_in_sequence X batch_size X input_features\n",
    "        batch = torch.zeros((days_in_sequence, batch_size, input_features), dtype=torch.float64)\n",
    "        \n",
    "        # Fill in batch tensor\n",
    "        for batch_col in range(0, batch_size):\n",
    "            # Create sequence -- grab horse data from each race -- and add to batch\n",
    "            sequence = torch.tensor([batch_data[i][batch_col] for i in range(0, days_in_sequence)])\n",
    "            batch[:, batch_col] = sequence\n",
    "            \n",
    "        # Create label tensor\n",
    "        label_tensor = torch.tensor(batch_labels[-1], dtype=torch.float64)\n",
    "        \n",
    "        yield batch, label_tensor\n",
    "    \n",
    "    \n",
    "test = dataloader(input_data, labels)\n",
    "\n",
    "sample_batch, sample_label = next(iter(dataloader(input_data, labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandicappingBrain(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_length=6,lstm_size=64, lstm_layers=1, output_size=1, \n",
    "                               drop_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.input_length = input_length\n",
    "        self.output_size = output_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        ## LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_length, lstm_size, lstm_layers, \n",
    "                            dropout=drop_prob, batch_first=False)\n",
    "        \n",
    "        ## Dropout Layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## Fully-connected Output Layer\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "      \n",
    "    \n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        '''\n",
    "            Perform a forward pass through the network\n",
    "            \n",
    "            Args:\n",
    "                nn_input: the batch of input to NN\n",
    "                hidden_state: The LSTM hidden/cell state tuple\n",
    "                \n",
    "            Returns:\n",
    "                logps: log softmax output\n",
    "                hidden_state: the updated hidden/cell state tuple\n",
    "        '''\n",
    "        # Input -> LSTM\n",
    "        lstm_out, hidden_state = self.lstm(nn_input, hidden_state)\n",
    "\n",
    "        # Stack up LSTM outputs -- this gets the final LSTM output for each sequence in the batch\n",
    "        lstm_out = lstm_out[-1, :, :]\n",
    "        \n",
    "        # LSTM -> Dense Layer\n",
    "        dense_out = self.dropout(self.fc(lstm_out))\n",
    "        \n",
    "        # Return the final output and the hidden state\n",
    "        return dense_out, hidden_state\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "              weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8672],\n",
      "        [-0.8672],\n",
      "        [-0.8672],\n",
      "        [-0.8672],\n",
      "        [-0.8672],\n",
      "        [-0.8672],\n",
      "        [-0.8672],\n",
      "        [-0.8672],\n",
      "        [-0.8672],\n",
      "        [-0.0000]], dtype=torch.float64, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nate/anaconda3/envs/julie-stav-ws/lib/python3.5/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "test_model = HandicappingBrain(input_length=6, lstm_size=8, lstm_layers=1, drop_prob=0.2, output_size=1).double()\n",
    "hidden = test_model.init_hidden(10)\n",
    "dense_out, _ = test_model.forward(sample_batch, hidden)\n",
    "print(dense_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Test/Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 94\n",
      "23 23\n"
     ]
    }
   ],
   "source": [
    "test_prop = 0.2\n",
    "test_end_idx = int(len(input_data[0]) * test_prop)\n",
    "\n",
    "# Create test set -- test_prob% of our total data set\n",
    "test_data = [race[:test_end_idx] for race in input_data]\n",
    "test_labels = [race[:test_end_idx] for race in labels]\n",
    "\n",
    "# Craete training set\n",
    "train_data = [race[test_end_idx:] for race in input_data]\n",
    "train_labels = [race[test_end_idx:] for race in labels]\n",
    "\n",
    "print(len(train_data[0]), len(train_labels[0]))\n",
    "print(len(test_data[0]), len(test_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandicappingBrain(\n",
      "  (lstm): LSTM(6, 32, num_layers=2, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Train on GPU if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define model -- set dtype to double since our data requires it\n",
    "model = HandicappingBrain(input_length=6, lstm_size=32, lstm_layers=2, output_size=1, drop_prob=0.3).double()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Epoch 1/100... Training Loss 7909.271400... Validation Loss: 8193.116508...\n",
      "Epoch 1/100... Training Loss 1375.658125... Validation Loss: 8166.652865...\n",
      "Epoch 1/100... Training Loss 5399.765759... Validation Loss: 8140.496639...\n",
      "Epoch 1/100... Training Loss 5810.713351... Validation Loss: 8107.723536...\n",
      "Starting Epoch 2\n",
      "Epoch 2/100... Training Loss 7746.765515... Validation Loss: 8046.034923...\n",
      "Epoch 2/100... Training Loss 1297.336616... Validation Loss: 8000.553643...\n",
      "Epoch 2/100... Training Loss 5186.975356... Validation Loss: 7950.682108...\n",
      "Epoch 2/100... Training Loss 5689.454335... Validation Loss: 7895.253270...\n",
      "Starting Epoch 3\n",
      "Epoch 3/100... Training Loss 7512.273524... Validation Loss: 7805.591061...\n",
      "Epoch 3/100... Training Loss 1200.033970... Validation Loss: 7745.649674...\n",
      "Epoch 3/100... Training Loss 5054.828572... Validation Loss: 7685.168238...\n",
      "Epoch 3/100... Training Loss 5379.201314... Validation Loss: 7624.179281...\n",
      "Starting Epoch 4\n",
      "Epoch 4/100... Training Loss 7057.652191... Validation Loss: 7528.640775...\n",
      "Epoch 4/100... Training Loss 1087.953007... Validation Loss: 7468.301609...\n",
      "Epoch 4/100... Training Loss 4740.190981... Validation Loss: 7411.908974...\n",
      "Epoch 4/100... Training Loss 5026.309278... Validation Loss: 7356.915432...\n",
      "Starting Epoch 5\n",
      "Epoch 5/100... Training Loss 6932.628329... Validation Loss: 7277.320989...\n",
      "Epoch 5/100... Training Loss 1138.444459... Validation Loss: 7227.008280...\n",
      "Epoch 5/100... Training Loss 4540.815380... Validation Loss: 7177.955742...\n",
      "Epoch 5/100... Training Loss 5033.190089... Validation Loss: 7131.817519...\n",
      "Starting Epoch 6\n",
      "Epoch 6/100... Training Loss 6542.699106... Validation Loss: 7066.984807...\n",
      "Epoch 6/100... Training Loss 1092.823548... Validation Loss: 7025.411276...\n",
      "Epoch 6/100... Training Loss 4761.469866... Validation Loss: 6984.795362...\n",
      "Epoch 6/100... Training Loss 5308.543463... Validation Loss: 6944.953566...\n",
      "Starting Epoch 7\n",
      "Epoch 7/100... Training Loss 6655.388387... Validation Loss: 6886.380895...\n",
      "Epoch 7/100... Training Loss 927.818387... Validation Loss: 6847.987020...\n",
      "Epoch 7/100... Training Loss 4364.198336... Validation Loss: 6810.044812...\n",
      "Epoch 7/100... Training Loss 4264.879350... Validation Loss: 6772.505081...\n",
      "Starting Epoch 8\n",
      "Epoch 8/100... Training Loss 6305.188964... Validation Loss: 6716.884006...\n",
      "Epoch 8/100... Training Loss 739.220546... Validation Loss: 6680.223651...\n",
      "Epoch 8/100... Training Loss 4238.399837... Validation Loss: 6643.871820...\n",
      "Epoch 8/100... Training Loss 4799.064977... Validation Loss: 6607.817272...\n",
      "Starting Epoch 9\n",
      "Epoch 9/100... Training Loss 6345.404803... Validation Loss: 6554.255255...\n",
      "Epoch 9/100... Training Loss 914.466560... Validation Loss: 6518.799269...\n",
      "Epoch 9/100... Training Loss 4120.348650... Validation Loss: 6483.611842...\n",
      "Epoch 9/100... Training Loss 5088.941069... Validation Loss: 6448.713247...\n",
      "Starting Epoch 10\n",
      "Epoch 10/100... Training Loss 6198.759048... Validation Loss: 6396.786480...\n",
      "Epoch 10/100... Training Loss 799.212485... Validation Loss: 6362.440521...\n",
      "Epoch 10/100... Training Loss 4003.234655... Validation Loss: 6328.297149...\n",
      "Epoch 10/100... Training Loss 4809.162220... Validation Loss: 6294.350747...\n",
      "Starting Epoch 11\n",
      "Epoch 11/100... Training Loss 6060.873965... Validation Loss: 6243.786302...\n",
      "Epoch 11/100... Training Loss 1025.460916... Validation Loss: 6210.307466...\n",
      "Epoch 11/100... Training Loss 3676.961758... Validation Loss: 6177.009539...\n",
      "Epoch 11/100... Training Loss 4268.513273... Validation Loss: 6143.883992...\n",
      "Starting Epoch 12\n",
      "Epoch 12/100... Training Loss 5645.910248... Validation Loss: 6094.512049...\n",
      "Epoch 12/100... Training Loss 730.480794... Validation Loss: 6061.800410...\n",
      "Epoch 12/100... Training Loss 3792.304371... Validation Loss: 6029.246081...\n",
      "Epoch 12/100... Training Loss 4405.595910... Validation Loss: 5996.846670...\n",
      "Starting Epoch 13\n",
      "Epoch 13/100... Training Loss 5499.001941... Validation Loss: 5948.527229...\n",
      "Epoch 13/100... Training Loss 797.485184... Validation Loss: 5916.492441...\n",
      "Epoch 13/100... Training Loss 3934.427258... Validation Loss: 5884.602570...\n",
      "Epoch 13/100... Training Loss 4055.807326... Validation Loss: 5852.856592...\n",
      "Starting Epoch 14\n",
      "Epoch 14/100... Training Loss 5665.512398... Validation Loss: 5805.498597...\n",
      "Epoch 14/100... Training Loss 674.548445... Validation Loss: 5774.096248...\n",
      "Epoch 14/100... Training Loss 3849.981112... Validation Loss: 5742.826978...\n",
      "Epoch 14/100... Training Loss 3679.066794... Validation Loss: 5711.689546...\n",
      "Starting Epoch 15\n",
      "Epoch 15/100... Training Loss 5535.701145... Validation Loss: 5665.224798...\n",
      "Epoch 15/100... Training Loss 443.619572... Validation Loss: 5634.405230...\n",
      "Epoch 15/100... Training Loss 4326.139406... Validation Loss: 5603.708427...\n",
      "Epoch 15/100... Training Loss 3568.300719... Validation Loss: 5573.133738...\n",
      "Starting Epoch 16\n",
      "Epoch 16/100... Training Loss 5771.177879... Validation Loss: 5527.497416...\n",
      "Epoch 16/100... Training Loss 522.708562... Validation Loss: 5497.221698...\n",
      "Epoch 16/100... Training Loss 3983.123629... Validation Loss: 5467.062733...\n",
      "Epoch 16/100... Training Loss 4062.886166... Validation Loss: 5437.020004...\n",
      "Starting Epoch 17\n",
      "Epoch 17/100... Training Loss 6413.800850... Validation Loss: 5392.167756...\n",
      "Epoch 17/100... Training Loss 501.671975... Validation Loss: 5362.400158...\n",
      "Epoch 17/100... Training Loss 3920.121122... Validation Loss: 5332.745758...\n",
      "Epoch 17/100... Training Loss 3984.039182... Validation Loss: 5303.202836...\n",
      "Starting Epoch 18\n",
      "Epoch 18/100... Training Loss 4407.672634... Validation Loss: 5259.094798...\n",
      "Epoch 18/100... Training Loss 482.995275... Validation Loss: 5229.828153...\n",
      "Epoch 18/100... Training Loss 2597.097843... Validation Loss: 5200.669650...\n",
      "Epoch 18/100... Training Loss 5219.614294... Validation Loss: 5171.618497...\n",
      "Starting Epoch 19\n",
      "Epoch 19/100... Training Loss 4264.843671... Validation Loss: 5128.244236...\n",
      "Epoch 19/100... Training Loss 467.542850... Validation Loss: 5099.460647...\n",
      "Epoch 19/100... Training Loss 3796.777864... Validation Loss: 5070.781870...\n",
      "Epoch 19/100... Training Loss 2809.830662... Validation Loss: 5042.207496...\n",
      "Starting Epoch 20\n",
      "Epoch 20/100... Training Loss 4119.835787... Validation Loss: 4999.540076...\n",
      "Epoch 20/100... Training Loss 799.935230... Validation Loss: 4971.221934...\n",
      "Epoch 20/100... Training Loss 3059.470393... Validation Loss: 4943.005437...\n",
      "Epoch 20/100... Training Loss 2702.385734... Validation Loss: 4914.890243...\n",
      "Starting Epoch 21\n",
      "Epoch 21/100... Training Loss 4417.309852... Validation Loss: 4872.907262...\n",
      "Epoch 21/100... Training Loss 677.549645... Validation Loss: 4845.043062...\n",
      "Epoch 21/100... Training Loss 2633.311436... Validation Loss: 4817.278812...\n",
      "Epoch 21/100... Training Loss 2962.871071... Validation Loss: 4789.613182...\n",
      "Starting Epoch 22\n",
      "Epoch 22/100... Training Loss 5194.402527... Validation Loss: 4748.296949...\n",
      "Epoch 22/100... Training Loss 321.044557... Validation Loss: 4720.873307...\n",
      "Epoch 22/100... Training Loss 2544.325654... Validation Loss: 4693.548321...\n",
      "Epoch 22/100... Training Loss 2495.664456... Validation Loss: 4666.320710...\n",
      "Starting Epoch 23\n",
      "Epoch 23/100... Training Loss 5111.552832... Validation Loss: 4625.659015...\n",
      "Epoch 23/100... Training Loss 904.407918... Validation Loss: 4598.669881...\n",
      "Epoch 23/100... Training Loss 2093.893398... Validation Loss: 4575.276643...\n",
      "Epoch 23/100... Training Loss 3946.761026... Validation Loss: 4551.281635...\n",
      "Starting Epoch 24\n",
      "Epoch 24/100... Training Loss 5030.867596... Validation Loss: 4514.444920...\n",
      "Epoch 24/100... Training Loss 314.719666... Validation Loss: 4490.200087...\n",
      "Epoch 24/100... Training Loss 2394.586822... Validation Loss: 4471.274006...\n",
      "Epoch 24/100... Training Loss 3901.052402... Validation Loss: 4450.938381...\n",
      "Starting Epoch 25\n",
      "Epoch 25/100... Training Loss 3497.322819... Validation Loss: 4418.408829...\n",
      "Epoch 25/100... Training Loss 550.867378... Validation Loss: 4398.327657...\n",
      "Epoch 25/100... Training Loss 3108.446589... Validation Loss: 4384.469164...\n",
      "Epoch 25/100... Training Loss 2255.583439... Validation Loss: 4368.280017...\n",
      "Starting Epoch 26\n",
      "Epoch 26/100... Training Loss 5399.872722... Validation Loss: 4340.616807...\n",
      "Epoch 26/100... Training Loss 317.471952... Validation Loss: 4323.045253...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100... Training Loss 3069.849293... Validation Loss: 4311.185507...\n",
      "Epoch 26/100... Training Loss 3833.194950... Validation Loss: 4296.663656...\n",
      "Starting Epoch 27\n",
      "Epoch 27/100... Training Loss 4351.614111... Validation Loss: 4271.008350...\n",
      "Epoch 27/100... Training Loss 554.810043... Validation Loss: 4254.492704...\n",
      "Epoch 27/100... Training Loss 2640.510853... Validation Loss: 4243.460896...\n",
      "Epoch 27/100... Training Loss 3390.586659... Validation Loss: 4229.654758...\n",
      "Starting Epoch 28\n",
      "Epoch 28/100... Training Loss 4814.974627... Validation Loss: 4204.909851...\n",
      "Epoch 28/100... Training Loss 674.550382... Validation Loss: 4188.889226...\n",
      "Epoch 28/100... Training Loss 2603.575869... Validation Loss: 4178.235371...\n",
      "Epoch 28/100... Training Loss 2938.060173... Validation Loss: 4164.783312...\n",
      "Starting Epoch 29\n",
      "Epoch 29/100... Training Loss 4249.804817... Validation Loss: 4140.530926...\n",
      "Epoch 29/100... Training Loss 561.572529... Validation Loss: 4124.793568...\n",
      "Epoch 29/100... Training Loss 1342.820079... Validation Loss: 4114.345256...\n",
      "Epoch 29/100... Training Loss 3325.677537... Validation Loss: 4101.104255...\n",
      "Starting Epoch 30\n",
      "Epoch 30/100... Training Loss 4722.194363... Validation Loss: 4077.177734...\n",
      "Epoch 30/100... Training Loss 681.493462... Validation Loss: 4061.639093...\n",
      "Epoch 30/100... Training Loss 2119.606014... Validation Loss: 4051.332934...\n",
      "Epoch 30/100... Training Loss 2434.930980... Validation Loss: 4038.252107...\n",
      "Starting Epoch 31\n",
      "Epoch 31/100... Training Loss 3613.636590... Validation Loss: 4014.591717...\n",
      "Epoch 31/100... Training Loss 456.298906... Validation Loss: 3999.220385...\n",
      "Epoch 31/100... Training Loss 2919.316232... Validation Loss: 3989.025569...\n",
      "Epoch 31/100... Training Loss 3699.944037... Validation Loss: 3976.078773...\n",
      "Starting Epoch 32\n",
      "Epoch 32/100... Training Loss 3560.715995... Validation Loss: 3952.653184...\n",
      "Epoch 32/100... Training Loss 691.094084... Validation Loss: 3937.432946...\n",
      "Epoch 32/100... Training Loss 2890.026424... Validation Loss: 3927.339651...\n",
      "Epoch 32/100... Training Loss 2355.688316... Validation Loss: 3914.518488...\n",
      "Starting Epoch 33\n",
      "Epoch 33/100... Training Loss 5147.139195... Validation Loss: 3891.315279...\n",
      "Epoch 33/100... Training Loss 244.672328... Validation Loss: 3876.238093...\n",
      "Epoch 33/100... Training Loss 2863.341000... Validation Loss: 3866.239916...\n",
      "Epoch 33/100... Training Loss 3206.309562... Validation Loss: 3853.539839...\n",
      "Starting Epoch 34\n",
      "Epoch 34/100... Training Loss 3452.337057... Validation Loss: 3830.556855...\n",
      "Epoch 34/100... Training Loss 702.984408... Validation Loss: 3815.622870...\n",
      "Epoch 34/100... Training Loss 1972.310935... Validation Loss: 3805.720444...\n",
      "Epoch 34/100... Training Loss 2728.965579... Validation Loss: 3793.140938...\n",
      "Starting Epoch 35\n",
      "Epoch 35/100... Training Loss 3964.762879... Validation Loss: 3770.374592...\n",
      "Epoch 35/100... Training Loss 488.279092... Validation Loss: 3755.580375...\n",
      "Epoch 35/100... Training Loss 1502.964673... Validation Loss: 3745.769637...\n",
      "Epoch 35/100... Training Loss 2242.553404... Validation Loss: 3733.306458...\n",
      "Starting Epoch 36\n",
      "Epoch 36/100... Training Loss 3919.302391... Validation Loss: 3710.750882...\n",
      "Epoch 36/100... Training Loss 607.996852... Validation Loss: 3696.093675...\n",
      "Epoch 36/100... Training Loss 2345.324638... Validation Loss: 3686.373954...\n",
      "Epoch 36/100... Training Loss 2206.483006... Validation Loss: 3674.026725...\n",
      "Starting Epoch 37\n",
      "Epoch 37/100... Training Loss 3297.332485... Validation Loss: 3651.682733...\n",
      "Epoch 37/100... Training Loss 400.639461... Validation Loss: 3637.164818...\n",
      "Epoch 37/100... Training Loss 3207.362121... Validation Loss: 3627.538574...\n",
      "Epoch 37/100... Training Loss 2634.739401... Validation Loss: 3615.309640...\n",
      "Starting Epoch 38\n",
      "Epoch 38/100... Training Loss 2092.257248... Validation Loss: 3593.177849...\n",
      "Epoch 38/100... Training Loss 521.001697... Validation Loss: 3578.796012...\n",
      "Epoch 38/100... Training Loss 943.443946... Validation Loss: 3569.260009...\n",
      "Epoch 38/100... Training Loss 2139.009206... Validation Loss: 3557.144604...\n",
      "Starting Epoch 39\n",
      "Epoch 39/100... Training Loss 4372.430580... Validation Loss: 3535.218567...\n",
      "Epoch 39/100... Training Loss 638.602829... Validation Loss: 3520.971620...\n",
      "Epoch 39/100... Training Loss 4070.860901... Validation Loss: 3511.524495...\n",
      "Epoch 39/100... Training Loss 2104.811822... Validation Loss: 3499.522975...\n",
      "Starting Epoch 40\n",
      "Epoch 40/100... Training Loss 4334.867355... Validation Loss: 3477.802437...\n",
      "Epoch 40/100... Training Loss 650.252454... Validation Loss: 3463.688756...\n",
      "Epoch 40/100... Training Loss 2234.716385... Validation Loss: 3454.330322...\n",
      "Epoch 40/100... Training Loss 2547.604245... Validation Loss: 3442.442293...\n",
      "Starting Epoch 41\n",
      "Epoch 41/100... Training Loss 3105.632209... Validation Loss: 3420.929660...\n",
      "Epoch 41/100... Training Loss 866.159624... Validation Loss: 3406.951875...\n",
      "Epoch 41/100... Training Loss 2668.833352... Validation Loss: 3397.684107...\n",
      "Epoch 41/100... Training Loss 2520.206804... Validation Loss: 3385.910782...\n",
      "Starting Epoch 42\n",
      "Epoch 42/100... Training Loss 4271.797686... Validation Loss: 3364.601525...\n",
      "Epoch 42/100... Training Loss 675.772140... Validation Loss: 3350.755542...\n",
      "Epoch 42/100... Training Loss 2185.850550... Validation Loss: 3341.573901...\n",
      "Epoch 42/100... Training Loss 2009.608009... Validation Loss: 3329.910310...\n",
      "Starting Epoch 43\n",
      "Epoch 43/100... Training Loss 4231.522694... Validation Loss: 3308.804819...\n",
      "Epoch 43/100... Training Loss 689.272590... Validation Loss: 3295.092104...\n",
      "Epoch 43/100... Training Loss 3093.043481... Validation Loss: 3285.999128...\n",
      "Epoch 43/100... Training Loss 1980.671729... Validation Loss: 3274.447737...\n",
      "Starting Epoch 44\n",
      "Epoch 44/100... Training Loss 4200.690883... Validation Loss: 3253.543470...\n",
      "Epoch 44/100... Training Loss 703.938453... Validation Loss: 3239.962613...\n",
      "Epoch 44/100... Training Loss 2138.728469... Validation Loss: 3230.955464...\n",
      "Epoch 44/100... Training Loss 970.104389... Validation Loss: 3219.515140...\n",
      "Starting Epoch 45\n",
      "Epoch 45/100... Training Loss 3546.793655... Validation Loss: 3198.814000...\n",
      "Epoch 45/100... Training Loss 719.088675... Validation Loss: 3185.365189...\n",
      "Epoch 45/100... Training Loss 1164.822224... Validation Loss: 3176.448094...\n",
      "Epoch 45/100... Training Loss 2912.098128... Validation Loss: 3165.119806...\n",
      "Starting Epoch 46\n",
      "Epoch 46/100... Training Loss 2883.480292... Validation Loss: 3144.620408...\n",
      "Epoch 46/100... Training Loss 643.837916... Validation Loss: 3131.302767...\n",
      "Epoch 46/100... Training Loss 1613.564336... Validation Loss: 3122.576490...\n",
      "Epoch 46/100... Training Loss 1397.898073... Validation Loss: 3111.269024...\n",
      "Starting Epoch 47\n",
      "Epoch 47/100... Training Loss 2841.754413... Validation Loss: 3090.989075...\n",
      "Epoch 47/100... Training Loss 662.910735... Validation Loss: 3077.810366...\n",
      "Epoch 47/100... Training Loss 2549.659138... Validation Loss: 3069.073491...\n",
      "Epoch 47/100... Training Loss 2870.823994... Validation Loss: 3057.971870...\n",
      "Starting Epoch 48\n",
      "Epoch 48/100... Training Loss 4073.901081... Validation Loss: 3037.879670...\n",
      "Epoch 48/100... Training Loss 682.932740... Validation Loss: 3024.823906...\n",
      "Epoch 48/100... Training Loss 2049.015014... Validation Loss: 3016.165718...\n",
      "Epoch 48/100... Training Loss 1339.211079... Validation Loss: 3005.168145...\n",
      "Starting Epoch 49\n",
      "Epoch 49/100... Training Loss 2122.110484... Validation Loss: 2985.269068...\n",
      "Epoch 49/100... Training Loss 871.387363... Validation Loss: 2972.341216...\n",
      "Epoch 49/100... Training Loss 2514.014162... Validation Loss: 2963.765488...\n",
      "Epoch 49/100... Training Loss 2832.921606... Validation Loss: 2952.874110...\n",
      "Starting Epoch 50\n",
      "Epoch 50/100... Training Loss 3364.851536... Validation Loss: 2933.168985...\n",
      "Epoch 50/100... Training Loss 806.130041... Validation Loss: 2920.368438...\n",
      "Epoch 50/100... Training Loss 2499.185852... Validation Loss: 2911.881324...\n",
      "Epoch 50/100... Training Loss 2304.824927... Validation Loss: 2901.101609...\n",
      "Starting Epoch 51\n",
      "Epoch 51/100... Training Loss 4636.803860... Validation Loss: 2881.596920...\n",
      "Epoch 51/100... Training Loss 746.997170... Validation Loss: 2868.928261...\n",
      "Epoch 51/100... Training Loss 2973.800058... Validation Loss: 2860.529510...\n",
      "Epoch 51/100... Training Loss 3824.072382... Validation Loss: 2849.860481...\n",
      "Starting Epoch 52\n",
      "Epoch 52/100... Training Loss 2643.580081... Validation Loss: 2830.556563...\n",
      "Epoch 52/100... Training Loss 1148.180315... Validation Loss: 2818.016871...\n",
      "Epoch 52/100... Training Loss 2467.873362... Validation Loss: 2809.703460...\n",
      "Epoch 52/100... Training Loss 2780.768881... Validation Loss: 2799.143614...\n",
      "Starting Epoch 53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100... Training Loss 3927.337683... Validation Loss: 2780.037359...\n",
      "Epoch 53/100... Training Loss 866.600726... Validation Loss: 2767.626370...\n",
      "Epoch 53/100... Training Loss 1460.517500... Validation Loss: 2759.399264...\n",
      "Epoch 53/100... Training Loss 2764.893282... Validation Loss: 2748.947608...\n",
      "Starting Epoch 54\n",
      "Epoch 54/100... Training Loss 2571.875556... Validation Loss: 2730.037218...\n",
      "Epoch 54/100... Training Loss 888.209214... Validation Loss: 2717.753707...\n",
      "Epoch 54/100... Training Loss 943.672260... Validation Loss: 2709.610637...\n",
      "Epoch 54/100... Training Loss 2227.947818... Validation Loss: 2699.267091...\n",
      "Starting Epoch 55\n",
      "Epoch 55/100... Training Loss 1868.286461... Validation Loss: 2680.551820...\n",
      "Epoch 55/100... Training Loss 844.080443... Validation Loss: 2668.395459...\n",
      "Epoch 55/100... Training Loss 1924.995638... Validation Loss: 2660.336882...\n",
      "Epoch 55/100... Training Loss 1686.595419... Validation Loss: 2650.100303...\n",
      "Starting Epoch 56\n",
      "Epoch 56/100... Training Loss 3173.587094... Validation Loss: 2631.580296...\n",
      "Epoch 56/100... Training Loss 870.269741... Validation Loss: 2619.550673...\n",
      "Epoch 56/100... Training Loss 401.172190... Validation Loss: 2611.576392...\n",
      "Epoch 56/100... Training Loss 2193.698011... Validation Loss: 2601.448300...\n",
      "Starting Epoch 57\n",
      "Epoch 57/100... Training Loss 2468.921616... Validation Loss: 2583.124199...\n",
      "Epoch 57/100... Training Loss 957.441398... Validation Loss: 2571.222166...\n",
      "Epoch 57/100... Training Loss 2905.187812... Validation Loss: 2563.332320...\n",
      "Epoch 57/100... Training Loss 2177.563716... Validation Loss: 2553.310765...\n",
      "Starting Epoch 58\n",
      "Epoch 58/100... Training Loss 5160.340853... Validation Loss: 2535.180911...\n",
      "Epoch 58/100... Training Loss 981.695632... Validation Loss: 2523.405543...\n",
      "Epoch 58/100... Training Loss 1881.735532... Validation Loss: 2515.600423...\n",
      "Epoch 58/100... Training Loss 2693.164741... Validation Loss: 2505.686623...\n",
      "Starting Epoch 59\n",
      "Epoch 59/100... Training Loss 3089.207003... Validation Loss: 2487.750687...\n",
      "Epoch 59/100... Training Loss 1006.853817... Validation Loss: 2476.101302...\n",
      "Epoch 59/100... Training Loss 2377.636187... Validation Loss: 2468.379443...\n",
      "Epoch 59/100... Training Loss 2147.901542... Validation Loss: 2458.571861...\n",
      "Starting Epoch 60\n",
      "Epoch 60/100... Training Loss 3060.406260... Validation Loss: 2440.829821...\n",
      "Epoch 60/100... Training Loss 983.874921... Validation Loss: 2429.307274...\n",
      "Epoch 60/100... Training Loss 1347.495723... Validation Loss: 2421.669634...\n",
      "Epoch 60/100... Training Loss 2668.567174... Validation Loss: 2411.968919...\n",
      "Starting Epoch 61\n",
      "Epoch 61/100... Training Loss 3728.549627... Validation Loss: 2394.420675...\n",
      "Epoch 61/100... Training Loss 968.831894... Validation Loss: 2383.023951...\n",
      "Epoch 61/100... Training Loss 1845.833434... Validation Loss: 2375.469852...\n",
      "Epoch 61/100... Training Loss 2657.286945... Validation Loss: 2365.875709...\n",
      "Starting Epoch 62\n",
      "Epoch 62/100... Training Loss 3705.174066... Validation Loss: 2348.520595...\n",
      "Epoch 62/100... Training Loss 1044.307670... Validation Loss: 2339.640709...\n",
      "Epoch 62/100... Training Loss 1322.800353... Validation Loss: 2334.101424...\n",
      "Epoch 62/100... Training Loss 2647.802584... Validation Loss: 2326.165473...\n",
      "Starting Epoch 63\n",
      "Epoch 63/100... Training Loss 2988.478234... Validation Loss: 2310.770590...\n",
      "Epoch 63/100... Training Loss 1031.883558... Validation Loss: 2303.948871...\n",
      "Epoch 63/100... Training Loss 2342.901851... Validation Loss: 2300.057333...\n",
      "Epoch 63/100... Training Loss 2101.059460... Validation Loss: 2293.480820...\n",
      "Starting Epoch 64\n",
      "Epoch 64/100... Training Loss 3679.797857... Validation Loss: 2279.698619...\n",
      "Epoch 64/100... Training Loss 1162.733087... Validation Loss: 2273.689211...\n",
      "Epoch 64/100... Training Loss 2337.489338... Validation Loss: 2270.442032...\n",
      "Epoch 64/100... Training Loss 1012.709441... Validation Loss: 2264.412701...\n",
      "Starting Epoch 65\n",
      "Epoch 65/100... Training Loss 2250.260854... Validation Loss: 2251.332878...\n",
      "Epoch 65/100... Training Loss 1079.157068... Validation Loss: 2245.740924...\n",
      "Epoch 65/100... Training Loss 784.643058... Validation Loss: 2243.095747...\n",
      "Epoch 65/100... Training Loss 2086.898590... Validation Loss: 2237.363771...\n",
      "Starting Epoch 66\n",
      "Epoch 66/100... Training Loss 2234.852865... Validation Loss: 2224.449214...\n",
      "Epoch 66/100... Training Loss 1223.831578... Validation Loss: 2218.940072...\n",
      "Epoch 66/100... Training Loss 1811.326507... Validation Loss: 2216.074071...\n",
      "Epoch 66/100... Training Loss 1538.173758... Validation Loss: 2210.399077...\n",
      "Starting Epoch 67\n",
      "Epoch 67/100... Training Loss 1510.244938... Validation Loss: 2197.783492...\n",
      "Epoch 67/100... Training Loss 1180.917456... Validation Loss: 2192.346445...\n",
      "Epoch 67/100... Training Loss 1288.956525... Validation Loss: 2189.664675...\n",
      "Epoch 67/100... Training Loss 2074.860804... Validation Loss: 2184.065359...\n",
      "Starting Epoch 68\n",
      "Epoch 68/100... Training Loss 2913.771520... Validation Loss: 2171.396636...\n",
      "Epoch 68/100... Training Loss 1199.310448... Validation Loss: 2165.995782...\n",
      "Epoch 68/100... Training Loss 1801.887622... Validation Loss: 2163.291104...\n",
      "Epoch 68/100... Training Loss 980.771696... Validation Loss: 2157.626502...\n",
      "Starting Epoch 69\n",
      "Epoch 69/100... Training Loss 3611.163662... Validation Loss: 2145.207408...\n",
      "Epoch 69/100... Training Loss 1240.308267... Validation Loss: 2139.865754...\n",
      "Epoch 69/100... Training Loss 240.990201... Validation Loss: 2137.103893...\n",
      "Epoch 69/100... Training Loss 2063.093534... Validation Loss: 2131.582814...\n",
      "Starting Epoch 70\n",
      "Epoch 70/100... Training Loss 2173.171230... Validation Loss: 2119.272209...\n",
      "Epoch 70/100... Training Loss 1235.988771... Validation Loss: 2113.970051...\n",
      "Epoch 70/100... Training Loss 1794.048832... Validation Loss: 2111.228398...\n",
      "Epoch 70/100... Training Loss 966.914254... Validation Loss: 2105.746620...\n",
      "Starting Epoch 71\n",
      "Epoch 71/100... Training Loss 5021.700060... Validation Loss: 2093.524604...\n",
      "Epoch 71/100... Training Loss 1237.741969... Validation Loss: 2088.261722...\n",
      "Epoch 71/100... Training Loss 3869.059707... Validation Loss: 2085.541808...\n",
      "Epoch 71/100... Training Loss 2599.205508... Validation Loss: 2080.101053...\n",
      "Starting Epoch 72\n",
      "Epoch 72/100... Training Loss 2861.209253... Validation Loss: 2067.967142...\n",
      "Epoch 72/100... Training Loss 1317.496372... Validation Loss: 2062.731096...\n",
      "Epoch 72/100... Training Loss 744.784709... Validation Loss: 2060.097588...\n",
      "Epoch 72/100... Training Loss 954.402925... Validation Loss: 2054.749830...\n",
      "Starting Epoch 73\n",
      "Epoch 73/100... Training Loss 2850.255231... Validation Loss: 2042.664412...\n",
      "Epoch 73/100... Training Loss 1270.142809... Validation Loss: 2037.510630...\n",
      "Epoch 73/100... Training Loss 1782.331891... Validation Loss: 2034.895796...\n",
      "Epoch 73/100... Training Loss 1496.180421... Validation Loss: 2029.580173...\n",
      "Starting Epoch 74\n",
      "Epoch 74/100... Training Loss 2836.493362... Validation Loss: 2017.563591...\n",
      "Epoch 74/100... Training Loss 1304.205515... Validation Loss: 2012.430619...\n",
      "Epoch 74/100... Training Loss 1779.085456... Validation Loss: 2009.866152...\n",
      "Epoch 74/100... Training Loss 2039.657603... Validation Loss: 2004.594509...\n",
      "Starting Epoch 75\n",
      "Epoch 75/100... Training Loss 4993.691389... Validation Loss: 1992.788096...\n",
      "Epoch 75/100... Training Loss 1318.680236... Validation Loss: 1987.774845...\n",
      "Epoch 75/100... Training Loss 1776.297834... Validation Loss: 1985.233458...\n",
      "Epoch 75/100... Training Loss 2584.288417... Validation Loss: 1979.720431...\n",
      "Starting Epoch 76\n",
      "Epoch 76/100... Training Loss 2087.503381... Validation Loss: 1967.822966...\n",
      "Epoch 76/100... Training Loss 1357.910444... Validation Loss: 1962.752229...\n",
      "Epoch 76/100... Training Loss 207.317714... Validation Loss: 1960.241074...\n",
      "Epoch 76/100... Training Loss 933.255094... Validation Loss: 1954.997669...\n",
      "Starting Epoch 77\n",
      "Epoch 77/100... Training Loss 3528.275458... Validation Loss: 1943.205149...\n",
      "Epoch 77/100... Training Loss 1371.590016... Validation Loss: 1938.166871...\n",
      "Epoch 77/100... Training Loss 1771.560890... Validation Loss: 1935.560193...\n",
      "Epoch 77/100... Training Loss 2578.098664... Validation Loss: 1930.354233...\n",
      "Starting Epoch 78\n",
      "Epoch 78/100... Training Loss 2791.295925... Validation Loss: 1918.776258...\n",
      "Epoch 78/100... Training Loss 1398.344802... Validation Loss: 1913.759747...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100... Training Loss 723.783786... Validation Loss: 1911.144083...\n",
      "Epoch 78/100... Training Loss 2024.990239... Validation Loss: 1905.968690...\n",
      "Starting Epoch 79\n",
      "Epoch 79/100... Training Loss 2779.294675... Validation Loss: 1894.428819...\n",
      "Epoch 79/100... Training Loss 1419.113991... Validation Loss: 1889.546516...\n",
      "Epoch 79/100... Training Loss 1767.555441... Validation Loss: 1886.998882...\n",
      "Epoch 79/100... Training Loss 3673.897663... Validation Loss: 1881.824394...\n",
      "Starting Epoch 80\n",
      "Epoch 80/100... Training Loss 4962.091415... Validation Loss: 1870.255484...\n",
      "Epoch 80/100... Training Loss 1425.748452... Validation Loss: 1865.199560...\n",
      "Epoch 80/100... Training Loss 1765.849030... Validation Loss: 1862.485375...\n",
      "Epoch 80/100... Training Loss 3121.279228... Validation Loss: 1857.231065...\n",
      "Starting Epoch 81\n",
      "Epoch 81/100... Training Loss 2757.595912... Validation Loss: 1845.576248...\n",
      "Epoch 81/100... Training Loss 1468.050741... Validation Loss: 1839.928749...\n",
      "Epoch 81/100... Training Loss 1240.938690... Validation Loss: 1836.723198...\n",
      "Epoch 81/100... Training Loss 913.891202... Validation Loss: 1831.027505...\n",
      "Starting Epoch 82\n",
      "Epoch 82/100... Training Loss 3480.803859... Validation Loss: 1819.091750...\n",
      "Epoch 82/100... Training Loss 1479.045576... Validation Loss: 1813.803396...\n",
      "Epoch 82/100... Training Loss 2286.997508... Validation Loss: 1811.082859...\n",
      "Epoch 82/100... Training Loss 1462.537185... Validation Loss: 1805.874793...\n",
      "Starting Epoch 83\n",
      "Epoch 83/100... Training Loss 2000.970719... Validation Loss: 1794.667703...\n",
      "Epoch 83/100... Training Loss 1521.768863... Validation Loss: 1790.036943...\n",
      "Epoch 83/100... Training Loss 1762.535606... Validation Loss: 1787.788303...\n",
      "Epoch 83/100... Training Loss 1460.344224... Validation Loss: 1782.999801...\n",
      "Starting Epoch 84\n",
      "Epoch 84/100... Training Loss 2728.412122... Validation Loss: 1772.108087...\n",
      "Epoch 84/100... Training Loss 1544.054396... Validation Loss: 1767.387678...\n",
      "Epoch 84/100... Training Loss 2285.858558... Validation Loss: 1765.118777...\n",
      "Epoch 84/100... Training Loss 2563.028547... Validation Loss: 1760.218873...\n",
      "Starting Epoch 85\n",
      "Epoch 85/100... Training Loss 1241.850361... Validation Loss: 1749.228808...\n",
      "Epoch 85/100... Training Loss 1568.377504... Validation Loss: 1744.583765...\n",
      "Epoch 85/100... Training Loss 1761.773321... Validation Loss: 1742.557555...\n",
      "Epoch 85/100... Training Loss 2009.338009... Validation Loss: 1738.149048...\n",
      "Starting Epoch 86\n",
      "Epoch 86/100... Training Loss 4188.980621... Validation Loss: 1727.796405...\n",
      "Epoch 86/100... Training Loss 1594.257275... Validation Loss: 1723.388699...\n",
      "Epoch 86/100... Training Loss 2809.468509... Validation Loss: 1722.184088...\n",
      "Epoch 86/100... Training Loss 2560.934323... Validation Loss: 1719.137454...\n",
      "Starting Epoch 87\n",
      "Epoch 87/100... Training Loss 2702.741027... Validation Loss: 1710.256754...\n",
      "Epoch 87/100... Training Loss 1609.048378... Validation Loss: 1706.685096...\n",
      "Epoch 87/100... Training Loss 1238.002647... Validation Loss: 1706.478874...\n",
      "Epoch 87/100... Training Loss 1455.059248... Validation Loss: 1704.833719...\n",
      "Starting Epoch 88\n",
      "Epoch 88/100... Training Loss 1956.451211... Validation Loss: 1697.721352...\n",
      "Epoch 88/100... Training Loss 1621.215408... Validation Loss: 1695.136318...\n",
      "Epoch 88/100... Training Loss 1761.986495... Validation Loss: 1695.992340...\n",
      "Epoch 88/100... Training Loss 2007.310878... Validation Loss: 1695.067357...\n",
      "Starting Epoch 89\n",
      "Epoch 89/100... Training Loss 1210.611406... Validation Loss: 1688.061890...\n",
      "Epoch 89/100... Training Loss 1648.079628... Validation Loss: 1686.015257...\n",
      "Epoch 89/100... Training Loss 1762.195972... Validation Loss: 1687.949609...\n",
      "Epoch 89/100... Training Loss 1454.528479... Validation Loss: 1689.338119...\n",
      "Starting Epoch 90\n",
      "Epoch 90/100... Training Loss 2694.095719... Validation Loss: 1684.104460...\n",
      "Epoch 90/100... Training Loss 1562.504388... Validation Loss: 1684.374687...\n",
      "Epoch 90/100... Training Loss 1762.003445... Validation Loss: 1706.004167...\n",
      "Epoch 90/100... Training Loss 906.517106... Validation Loss: 1721.239173...\n",
      "Starting Epoch 91\n",
      "Epoch 91/100... Training Loss 1213.941370... Validation Loss: 1689.761074...\n",
      "Epoch 91/100... Training Loss 1467.326683... Validation Loss: 1688.560724...\n",
      "Epoch 91/100... Training Loss 1762.083651... Validation Loss: 1696.240470...\n",
      "Epoch 91/100... Training Loss 3113.092490... Validation Loss: 1698.613299...\n",
      "Starting Epoch 92\n",
      "Epoch 92/100... Training Loss 1952.924756... Validation Loss: 1688.368746...\n",
      "Epoch 92/100... Training Loss 1473.105190... Validation Loss: 1685.678506...\n",
      "Epoch 92/100... Training Loss 2285.931076... Validation Loss: 1687.545493...\n",
      "Epoch 92/100... Training Loss 1454.458857... Validation Loss: 1686.906247...\n",
      "Starting Epoch 93\n",
      "Epoch 93/100... Training Loss 2691.317505... Validation Loss: 1677.703772...\n",
      "Epoch 93/100... Training Loss 1393.456188... Validation Loss: 1675.091485...\n",
      "Epoch 93/100... Training Loss 1238.467430... Validation Loss: 1692.744922...\n",
      "Epoch 93/100... Training Loss 2565.141718... Validation Loss: 1688.597980...\n",
      "Starting Epoch 94\n",
      "Epoch 94/100... Training Loss 1207.678441... Validation Loss: 1674.896368...\n",
      "Epoch 94/100... Training Loss 1470.197488... Validation Loss: 1673.521157...\n",
      "Epoch 94/100... Training Loss 1762.291428... Validation Loss: 1681.923367...\n",
      "Epoch 94/100... Training Loss 1454.662091... Validation Loss: 1684.698832...\n",
      "Starting Epoch 95\n",
      "Epoch 95/100... Training Loss 3435.206623... Validation Loss: 1672.721535...\n",
      "Epoch 95/100... Training Loss 1257.702104... Validation Loss: 1669.163772...\n",
      "Epoch 95/100... Training Loss 1238.914646... Validation Loss: 1673.830060...\n",
      "Epoch 95/100... Training Loss 1454.153308... Validation Loss: 1676.850171...\n",
      "Starting Epoch 96\n",
      "Epoch 96/100... Training Loss 4172.822656... Validation Loss: 1666.303109...\n",
      "Epoch 96/100... Training Loss 1113.123818... Validation Loss: 1661.346339...\n",
      "Epoch 96/100... Training Loss 1762.579322... Validation Loss: 1661.831080...\n",
      "Epoch 96/100... Training Loss 3112.185819... Validation Loss: 1660.874392...\n",
      "Starting Epoch 97\n",
      "Epoch 97/100... Training Loss 3423.305837... Validation Loss: 1650.482211...\n",
      "Epoch 97/100... Training Loss 1301.545562... Validation Loss: 1654.810925...\n",
      "Epoch 97/100... Training Loss 716.164019... Validation Loss: 1665.984909...\n",
      "Epoch 97/100... Training Loss 2013.585827... Validation Loss: 1670.190772...\n",
      "Starting Epoch 98\n",
      "Epoch 98/100... Training Loss 4165.641914... Validation Loss: 1661.082226...\n",
      "Epoch 98/100... Training Loss 990.947127... Validation Loss: 1641.243019...\n",
      "Epoch 98/100... Training Loss 1763.644610... Validation Loss: 1644.393392...\n",
      "Epoch 98/100... Training Loss 2006.196541... Validation Loss: 1639.834516...\n",
      "Starting Epoch 99\n",
      "Epoch 99/100... Training Loss 3419.369180... Validation Loss: 1631.624245...\n",
      "Epoch 99/100... Training Loss 1215.827127... Validation Loss: 1627.488322...\n",
      "Epoch 99/100... Training Loss 1763.566338... Validation Loss: 1639.781876...\n",
      "Epoch 99/100... Training Loss 2559.330860... Validation Loss: 1647.531792...\n",
      "Starting Epoch 100\n",
      "Epoch 100/100... Training Loss 1929.802204... Validation Loss: 1629.225033...\n",
      "Epoch 100/100... Training Loss 1126.537692... Validation Loss: 1622.932754...\n",
      "Epoch 100/100... Training Loss 1764.156941... Validation Loss: 1627.789077...\n",
      "Epoch 100/100... Training Loss 2008.124137... Validation Loss: 1632.528147...\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 10\n",
    "learning_rate = 0.003\n",
    "seq_length = days_in_sequence\n",
    "clip = 5\n",
    "input_length = 6\n",
    "\n",
    "print_every = 2\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "training_losses = [x for x in range(epochs)]\n",
    "validation_losses = [x for x in range(epochs)]\n",
    "\n",
    "# Set to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting Epoch {}'.format(epoch+1))\n",
    "    batches_processed = 0\n",
    "    \n",
    "    # Get batch data\n",
    "    for batch, labels in dataloader(train_data, train_labels,\n",
    "                                                input_features=input_length,\n",
    "                                                days_in_sequence=seq_length,\n",
    "                                                batch_size=batch_size):\n",
    "        # Increment step count\n",
    "        batches_processed += 1\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        train_hidden = model.init_hidden(batch_size)\n",
    "        train_hidden = tuple([each.data for each in train_hidden])\n",
    "        \n",
    "        # Set tensors to correct device\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        for each in train_hidden:\n",
    "            each.to(device)\n",
    "            \n",
    "        # Zero out gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Run batch data through model\n",
    "        train_out, train_hidden = model(batch, train_hidden)\n",
    "        \n",
    "        # Calculate loss and perform back propogation -- clip gradients if necessary\n",
    "        loss = criterion(train_out, labels)\n",
    "        if epoch == 100:\n",
    "            print(train_out)\n",
    "            break\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Take optimizer step to update model weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation #\n",
    "        if batches_processed % print_every == 0:\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            \n",
    "            # Iterate through test data to validate model performance\n",
    "            for val_batch, val_labels in dataloader(test_data, test_labels,\n",
    "                                                    input_features=input_length,\n",
    "                                                    days_in_sequence=seq_length,\n",
    "                                                    batch_size=batch_size):\n",
    "                # Initialize hidden state\n",
    "                val_hidden = model.init_hidden(batch_size)\n",
    "                val_hidden = tuple([each.data for each in val_hidden])\n",
    "                \n",
    "                # Set tensors to correct device\n",
    "                val_batch, val_labels = val_batch.to(device), val_labels.to(device)\n",
    "                for each in val_hidden:\n",
    "                    each.to(device)\n",
    "                    \n",
    "                # Run data through network\n",
    "                val_output, val_hidden = model(val_batch, val_hidden)\n",
    "                \n",
    "                # Calculate loss\n",
    "                val_loss = criterion(val_output, val_labels)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            # Print out metrics\n",
    "            print('Epoch {}/{}...'.format(epoch+1, epochs),\n",
    "                  'Training Loss {:.6f}...'.format(loss.item()),\n",
    "                  'Validation Loss: {:.6f}...'.format(np.mean(val_losses)))\n",
    "            \n",
    "            # Record metrics\n",
    "            training_losses[epoch] = loss.item()\n",
    "            validation_losses[epoch] = np.mean(val_losses)\n",
    "            \n",
    "            # Set model back to train\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Training/Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:julie-stav-ws]",
   "language": "python",
   "name": "conda-env-julie-stav-ws-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
