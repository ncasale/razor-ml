{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse DRF Files to get Relevant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_master_df(path, num_races):\n",
    "    '''\n",
    "        Generate the master dataframe from which we will create our training/testing data\n",
    "        \n",
    "        Args:\n",
    "            path (string): Path to directory containing DRF files to parse\n",
    "            num_races (int): Number of races to use in each sequence (how many races back\n",
    "                             are we looking?)\n",
    "        \n",
    "        Returns: Dataframe containing all data from each DRF concatted together\n",
    "    '''\n",
    "    # Cap num_races\n",
    "    num_races = min(num_races, 9) # Only have max of 9 prev race's data\n",
    "    \n",
    "    # Get all DRF files in data directory\n",
    "    filenames = [path+file for file in os.listdir(path) if file.endswith(\".DRF\")]\n",
    "    \n",
    "    # Iterate through each file and concat data to master df\n",
    "    master_df = None\n",
    "    for ii, file in enumerate(filenames): \n",
    "        if ii == 0:\n",
    "            # First pass through just create master df\n",
    "            df = pd.read_csv(file, header=None)\n",
    "            master_df = slice_df(df, num_races)\n",
    "        else:\n",
    "            # All other passes, append sliced dataframe to master\n",
    "            df = pd.read_csv(file, header=None)\n",
    "            df = slice_df(df, num_races)\n",
    "            master_df = master_df.append(df, ignore_index=True)\n",
    "            \n",
    "    # Drop all rows containing NaN values (these horses didn't have enough prev races)\n",
    "    return master_df.dropna().reset_index().drop(['index'], axis=1)\n",
    "\n",
    "def slice_df(df, num_races=3):\n",
    "    # Define columns to grab\n",
    "    column_ids = OrderedDict({\n",
    "        'horse_age': (46,47),\n",
    "        'days_since_prev_race': (266, 266+num_races),\n",
    "        'distance': (316, 316+num_races),\n",
    "        'num_entrants': (346, 346+num_races),\n",
    "        'post_position': (356, 356+num_races),\n",
    "        'weight': (506, 506+num_races),\n",
    "        'label': (1036, 1036+num_races) # Finish time\n",
    "    })\n",
    "\n",
    "    # Select all of our column ranges\n",
    "    rng = []\n",
    "    col_names = []\n",
    "    for k,v in column_ids.items():\n",
    "        # Append range to rng -- special case for single field\n",
    "        if v[1] - v[0] == 1:\n",
    "            for i in range(num_races):\n",
    "                rng += [v[0]]\n",
    "                col_names.append('{}_{}'.format(k, i))\n",
    "        else:\n",
    "            # Handle column ranges\n",
    "            rng += range(v[0],v[1])\n",
    "            for ii in range(v[0], v[1]):\n",
    "                col_names.append('{}_{}'.format(k, ii-v[0]))\n",
    "\n",
    "    # Slice df on columns\n",
    "    ret = df.loc[:, rng]\n",
    "    ret.columns = col_names\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horse_age_0</th>\n",
       "      <th>horse_age_1</th>\n",
       "      <th>horse_age_2</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>weight_0</th>\n",
       "      <th>weight_1</th>\n",
       "      <th>weight_2</th>\n",
       "      <th>days_since_prev_race_0</th>\n",
       "      <th>...</th>\n",
       "      <th>days_since_prev_race_2</th>\n",
       "      <th>post_position_0</th>\n",
       "      <th>post_position_1</th>\n",
       "      <th>post_position_2</th>\n",
       "      <th>num_entrants_0</th>\n",
       "      <th>num_entrants_1</th>\n",
       "      <th>num_entrants_2</th>\n",
       "      <th>distance_0</th>\n",
       "      <th>distance_1</th>\n",
       "      <th>distance_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>75.17</td>\n",
       "      <td>73.40</td>\n",
       "      <td>74.41</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>1320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>75.20</td>\n",
       "      <td>73.53</td>\n",
       "      <td>80.98</td>\n",
       "      <td>124.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>1430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>104.90</td>\n",
       "      <td>75.43</td>\n",
       "      <td>108.81</td>\n",
       "      <td>123.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>1830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>103.83</td>\n",
       "      <td>101.39</td>\n",
       "      <td>109.50</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>1760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>96.75</td>\n",
       "      <td>106.14</td>\n",
       "      <td>100.58</td>\n",
       "      <td>123.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>1870.0</td>\n",
       "      <td>1760.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   horse_age_0  horse_age_1  horse_age_2  label_0  label_1  label_2  weight_0  \\\n",
       "0            2            2            2    75.17    73.40    74.41     121.0   \n",
       "1            4            4            4    75.20    73.53    80.98     124.0   \n",
       "2            4            4            4   104.90    75.43   108.81     123.0   \n",
       "3            4            4            4   103.83   101.39   109.50     121.0   \n",
       "4            2            2            2    96.75   106.14   100.58     123.0   \n",
       "\n",
       "   weight_1  weight_2  days_since_prev_race_0     ...      \\\n",
       "0     121.0     123.0                    26.0     ...       \n",
       "1     119.0     124.0                    14.0     ...       \n",
       "2     121.0     120.0                   147.0     ...       \n",
       "3     121.0     122.0                    23.0     ...       \n",
       "4     123.0     118.0                    63.0     ...       \n",
       "\n",
       "   days_since_prev_race_2  post_position_0  post_position_1  post_position_2  \\\n",
       "0                    11.0              3.0              1.0              9.0   \n",
       "1                    29.0              3.0              5.0              8.0   \n",
       "2                    13.0              5.0              1.0              5.0   \n",
       "3                    14.0              4.0              2.0              6.0   \n",
       "4                    20.0              7.0              5.0              2.0   \n",
       "\n",
       "   num_entrants_0  num_entrants_1  num_entrants_2  distance_0  distance_1  \\\n",
       "0             9.0             7.0            10.0      1320.0      1320.0   \n",
       "1             8.0             8.0             8.0      1320.0      1320.0   \n",
       "2             7.0             9.0             8.0      1760.0      1320.0   \n",
       "3             5.0             5.0             7.0      1760.0      1760.0   \n",
       "4             7.0            12.0             8.0      1760.0      1870.0   \n",
       "\n",
       "   distance_2  \n",
       "0      1320.0  \n",
       "1      1430.0  \n",
       "2      1830.0  \n",
       "3      1760.0  \n",
       "4      1760.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days_in_sequence = 3\n",
    "master_df = generate_master_df('./input_files/', days_in_sequence)\n",
    "master_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloader\n",
    "Create a generator that can parse through the master dataframe, and create batches of training data. These batches will have the shape (days_in_sequence, batch_size, input_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Section off data by race -- list of tuples (race_num, data)\n",
    "race_data = []\n",
    "for ii in range(days_in_sequence):\n",
    "    # Match all collumns for this race\n",
    "    pattern = re.compile('.*_{}'.format(ii))\n",
    "    cols = [pattern.match(col).string for col in master_df.columns if pattern.match(col) != None]\n",
    "    # Get data from these columns\n",
    "    data = master_df.loc[:, cols]\n",
    "    # Rename columns\n",
    "    cols = [col[:-2] for col in cols]\n",
    "    data.columns = cols\n",
    "    # Append to race data\n",
    "    race_data.append((ii, data)) \n",
    "    \n",
    "# Break race_data into input_data and label_data\n",
    "input_data = []\n",
    "labels = []\n",
    "for race_tup in race_data:\n",
    "    input_data.append(race_tup[1].drop(['label'], axis=1).values)\n",
    "    labels.append(race_tup[1]['label'].values)\n",
    "    \n",
    "# Want data to go in reverse order (oldest races first)\n",
    "input_data.reverse()\n",
    "labels.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(data, labels, days_in_sequence=3, batch_size=10, input_features=6):\n",
    "    # Truncate data to ensure only full batches\n",
    "    num_horses = len(data[0])\n",
    "    cutoff = (num_horses//batch_size)*batch_size\n",
    "    trunc_data = [race[:cutoff] for race in data]\n",
    "    trunc_labels = [race[:cutoff] for race in labels]\n",
    "    \n",
    "    # Create our batches\n",
    "    for ii in range(0, cutoff, batch_size):\n",
    "        # Get data for this batch\n",
    "        batch_data = [race[ii:ii+batch_size] for race in trunc_data]\n",
    "        batch_labels = [race[ii: ii+batch_size] for race in trunc_labels]\n",
    "        \n",
    "        # Create batch tensor of correct size -- days_in_sequence X batch_size X input_features\n",
    "        batch = torch.zeros((days_in_sequence, batch_size, input_features), dtype=torch.float64)\n",
    "        \n",
    "        # Fill in batch tensor\n",
    "        for batch_col in range(0, batch_size):\n",
    "            # Create sequence -- grab horse data from each race -- and add to batch\n",
    "            sequence = torch.tensor([batch_data[i][batch_col] for i in range(0, days_in_sequence)])\n",
    "            batch[:, batch_col] = sequence\n",
    "            \n",
    "        # Create label tensor\n",
    "        label_tensor = torch.tensor(batch_labels, dtype=torch.float64)\n",
    "        label_tensor = label_tensor.view(-1)\n",
    "        \n",
    "        yield batch, label_tensor\n",
    "    \n",
    "    \n",
    "test = dataloader(input_data, labels)\n",
    "\n",
    "sample_batch, sample_label = next(iter(dataloader(input_data, labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandicappingBrain(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_length=6,lstm_size=64, lstm_layers=1, output_size=1, \n",
    "                               drop_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.input_length = input_length\n",
    "        self.output_size = output_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        ## LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_length, lstm_size, lstm_layers, \n",
    "                            dropout=drop_prob, batch_first=False)\n",
    "        \n",
    "        ## Dropout Layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## Fully-connected Output Layer\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "      \n",
    "    \n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        '''\n",
    "            Perform a forward pass through the network\n",
    "            \n",
    "            Args:\n",
    "                nn_input: the batch of input to NN\n",
    "                hidden_state: The LSTM hidden/cell state tuple\n",
    "                \n",
    "            Returns:\n",
    "                logps: log softmax output\n",
    "                hidden_state: the updated hidden/cell state tuple\n",
    "        '''\n",
    "        # Input -> LSTM\n",
    "        lstm_out, hidden_state = self.lstm(nn_input, hidden_state)\n",
    "\n",
    "        # Stack up LSTM outputs -- this gets the final LSTM output for each sequence in the batch\n",
    "        #lstm_out = lstm_out[-1, :, :]\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.lstm_size)\n",
    "        \n",
    "        # LSTM -> Dense Layer\n",
    "        dense_out = self.dropout(self.fc(lstm_out))\n",
    "        \n",
    "        # Return the final output and the hidden state\n",
    "        return dense_out, hidden_state\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "              weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 8])\n",
      "tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.3181],\n",
      "        [0.3181],\n",
      "        [0.3181],\n",
      "        [0.0000],\n",
      "        [0.3181],\n",
      "        [0.3181],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.4931],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.4931],\n",
      "        [0.4931],\n",
      "        [0.4931],\n",
      "        [0.4931],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.5199],\n",
      "        [0.5199],\n",
      "        [0.5199],\n",
      "        [0.5199],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.5199],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.5199]], dtype=torch.float64, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nate/anaconda3/envs/julie-stav-ws/lib/python3.5/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "test_model = HandicappingBrain(input_length=6, lstm_size=8, lstm_layers=1, drop_prob=0.2, output_size=1).double()\n",
    "hidden = test_model.init_hidden(10)\n",
    "dense_out, _ = test_model.forward(sample_batch, hidden)\n",
    "print(dense_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Test/Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 94\n",
      "23 23\n"
     ]
    }
   ],
   "source": [
    "test_prop = 0.2\n",
    "test_end_idx = int(len(input_data[0]) * test_prop)\n",
    "\n",
    "# Create test set -- test_prob% of our total data set\n",
    "test_data = [race[:test_end_idx] for race in input_data]\n",
    "test_labels = [race[:test_end_idx] for race in labels]\n",
    "\n",
    "# Craete training set\n",
    "train_data = [race[test_end_idx:] for race in input_data]\n",
    "train_labels = [race[test_end_idx:] for race in labels]\n",
    "\n",
    "print(len(train_data[0]), len(train_labels[0]))\n",
    "print(len(test_data[0]), len(test_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandicappingBrain(\n",
      "  (lstm): LSTM(6, 32, num_layers=2, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Train on GPU if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define model -- set dtype to double since our data requires it\n",
    "model = HandicappingBrain(input_length=6, lstm_size=32, lstm_layers=2, output_size=1, drop_prob=0.3).double()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Epoch 1/100... Training Loss 7190.404724... Validation Loss: 6809.704264...\n",
      "Epoch 1/100... Training Loss 1084.992283... Validation Loss: 6773.951295...\n",
      "Epoch 1/100... Training Loss 4919.439300... Validation Loss: 6738.572896...\n",
      "Epoch 1/100... Training Loss 4682.638615... Validation Loss: 6703.639137...\n",
      "Starting Epoch 2\n",
      "Epoch 2/100... Training Loss 7011.384230... Validation Loss: 6651.578541...\n",
      "Epoch 2/100... Training Loss 1010.590624... Validation Loss: 6617.116711...\n",
      "Epoch 2/100... Training Loss 4827.527129... Validation Loss: 6582.033368...\n",
      "Epoch 2/100... Training Loss 4629.016486... Validation Loss: 6547.216177...\n",
      "Starting Epoch 3\n",
      "Epoch 3/100... Training Loss 6714.202495... Validation Loss: 6493.808125...\n",
      "Epoch 3/100... Training Loss 937.439159... Validation Loss: 6456.726496...\n",
      "Epoch 3/100... Training Loss 4811.482465... Validation Loss: 6420.724394...\n",
      "Epoch 3/100... Training Loss 4473.354401... Validation Loss: 6387.605863...\n",
      "Starting Epoch 4\n",
      "Epoch 4/100... Training Loss 6560.126632... Validation Loss: 6338.337460...\n",
      "Epoch 4/100... Training Loss 957.522035... Validation Loss: 6305.636812...\n",
      "Epoch 4/100... Training Loss 4557.074944... Validation Loss: 6273.578458...\n",
      "Epoch 4/100... Training Loss 4483.381030... Validation Loss: 6241.517217...\n",
      "Starting Epoch 5\n",
      "Epoch 5/100... Training Loss 6338.717402... Validation Loss: 6193.642958...\n",
      "Epoch 5/100... Training Loss 853.734788... Validation Loss: 6161.597389...\n",
      "Epoch 5/100... Training Loss 4576.094058... Validation Loss: 6128.139724...\n",
      "Epoch 5/100... Training Loss 4208.929725... Validation Loss: 6093.791094...\n",
      "Starting Epoch 6\n",
      "Epoch 6/100... Training Loss 6570.598259... Validation Loss: 6049.285372...\n",
      "Epoch 6/100... Training Loss 906.934175... Validation Loss: 6018.836854...\n",
      "Epoch 6/100... Training Loss 4223.788193... Validation Loss: 5986.784612...\n",
      "Epoch 6/100... Training Loss 3955.872734... Validation Loss: 5955.534302...\n",
      "Starting Epoch 7\n",
      "Epoch 7/100... Training Loss 6031.729140... Validation Loss: 5910.659301...\n",
      "Epoch 7/100... Training Loss 775.384591... Validation Loss: 5880.277479...\n",
      "Epoch 7/100... Training Loss 4327.950102... Validation Loss: 5849.375921...\n",
      "Epoch 7/100... Training Loss 3870.244619... Validation Loss: 5818.882408...\n",
      "Starting Epoch 8\n",
      "Epoch 8/100... Training Loss 6126.908341... Validation Loss: 5773.343966...\n",
      "Epoch 8/100... Training Loss 833.533393... Validation Loss: 5743.295459...\n",
      "Epoch 8/100... Training Loss 4099.071121... Validation Loss: 5712.097566...\n",
      "Epoch 8/100... Training Loss 3811.400715... Validation Loss: 5682.221818...\n",
      "Starting Epoch 9\n",
      "Epoch 9/100... Training Loss 5776.518470... Validation Loss: 5639.201721...\n",
      "Epoch 9/100... Training Loss 895.540352... Validation Loss: 5610.318382...\n",
      "Epoch 9/100... Training Loss 3910.157378... Validation Loss: 5581.432733...\n",
      "Epoch 9/100... Training Loss 3791.773820... Validation Loss: 5552.676833...\n",
      "Starting Epoch 10\n",
      "Epoch 10/100... Training Loss 5546.524764... Validation Loss: 5509.672698...\n",
      "Epoch 10/100... Training Loss 579.316118... Validation Loss: 5481.077154...\n",
      "Epoch 10/100... Training Loss 4118.680502... Validation Loss: 5452.592653...\n",
      "Epoch 10/100... Training Loss 3476.223512... Validation Loss: 5424.191821...\n",
      "Starting Epoch 11\n",
      "Epoch 11/100... Training Loss 5826.846217... Validation Loss: 5381.789789...\n",
      "Epoch 11/100... Training Loss 731.409752... Validation Loss: 5353.641994...\n",
      "Epoch 11/100... Training Loss 3961.275927... Validation Loss: 5325.615977...\n",
      "Epoch 11/100... Training Loss 3266.885826... Validation Loss: 5297.744314...\n",
      "Starting Epoch 12\n",
      "Epoch 12/100... Training Loss 5288.908602... Validation Loss: 5256.297535...\n",
      "Epoch 12/100... Training Loss 532.704864... Validation Loss: 5228.898607...\n",
      "Epoch 12/100... Training Loss 3694.725592... Validation Loss: 5201.615813...\n",
      "Epoch 12/100... Training Loss 3827.509693... Validation Loss: 5174.505401...\n",
      "Starting Epoch 13\n",
      "Epoch 13/100... Training Loss 5337.448566... Validation Loss: 5134.136868...\n",
      "Epoch 13/100... Training Loss 685.328323... Validation Loss: 5107.297134...\n",
      "Epoch 13/100... Training Loss 3994.144640... Validation Loss: 5080.495046...\n",
      "Epoch 13/100... Training Loss 3645.156902... Validation Loss: 5053.816395...\n",
      "Starting Epoch 14\n",
      "Epoch 14/100... Training Loss 5118.564381... Validation Loss: 5014.005709...\n",
      "Epoch 14/100... Training Loss 638.543534... Validation Loss: 4987.600254...\n",
      "Epoch 14/100... Training Loss 3267.134220... Validation Loss: 4961.269625...\n",
      "Epoch 14/100... Training Loss 3489.650813... Validation Loss: 4935.104792...\n",
      "Starting Epoch 15\n",
      "Epoch 15/100... Training Loss 5114.049228... Validation Loss: 4896.139752...\n",
      "Epoch 15/100... Training Loss 633.092328... Validation Loss: 4870.361232...\n",
      "Epoch 15/100... Training Loss 3275.565737... Validation Loss: 4844.715023...\n",
      "Epoch 15/100... Training Loss 2985.534658... Validation Loss: 4819.203594...\n",
      "Starting Epoch 16\n",
      "Epoch 16/100... Training Loss 4986.733734... Validation Loss: 4781.128664...\n",
      "Epoch 16/100... Training Loss 702.418827... Validation Loss: 4755.876503...\n",
      "Epoch 16/100... Training Loss 3494.999599... Validation Loss: 4730.701785...\n",
      "Epoch 16/100... Training Loss 3313.917741... Validation Loss: 4705.594032...\n",
      "Starting Epoch 17\n",
      "Epoch 17/100... Training Loss 5316.732514... Validation Loss: 4668.135837...\n",
      "Epoch 17/100... Training Loss 581.142349... Validation Loss: 4643.265459...\n",
      "Epoch 17/100... Training Loss 3442.156962... Validation Loss: 4618.434155...\n",
      "Epoch 17/100... Training Loss 3574.808760... Validation Loss: 4593.701458...\n",
      "Starting Epoch 18\n",
      "Epoch 18/100... Training Loss 4994.183790... Validation Loss: 4556.801093...\n",
      "Epoch 18/100... Training Loss 465.871654... Validation Loss: 4532.313973...\n",
      "Epoch 18/100... Training Loss 3314.787288... Validation Loss: 4507.875567...\n",
      "Epoch 18/100... Training Loss 3354.740184... Validation Loss: 4483.561617...\n",
      "Starting Epoch 19\n",
      "Epoch 19/100... Training Loss 5047.504843... Validation Loss: 4447.273207...\n",
      "Epoch 19/100... Training Loss 493.421317... Validation Loss: 4423.210641...\n",
      "Epoch 19/100... Training Loss 3252.971504... Validation Loss: 4399.211920...\n",
      "Epoch 19/100... Training Loss 3096.228722... Validation Loss: 4375.336950...\n",
      "Starting Epoch 20\n",
      "Epoch 20/100... Training Loss 4607.723751... Validation Loss: 4339.695040...\n",
      "Epoch 20/100... Training Loss 523.759761... Validation Loss: 4316.029367...\n",
      "Epoch 20/100... Training Loss 3637.156750... Validation Loss: 4292.429157...\n",
      "Epoch 20/100... Training Loss 3576.432501... Validation Loss: 4268.939175...\n",
      "Starting Epoch 21\n",
      "Epoch 21/100... Training Loss 4802.395763... Validation Loss: 4233.905931...\n",
      "Epoch 21/100... Training Loss 322.596817... Validation Loss: 4210.629074...\n",
      "Epoch 21/100... Training Loss 2707.854566... Validation Loss: 4187.371130...\n",
      "Epoch 21/100... Training Loss 2239.756179... Validation Loss: 4164.207428...\n",
      "Starting Epoch 22\n",
      "Epoch 22/100... Training Loss 5140.342643... Validation Loss: 4129.643439...\n",
      "Epoch 22/100... Training Loss 433.199579... Validation Loss: 4106.655098...\n",
      "Epoch 22/100... Training Loss 3281.004302... Validation Loss: 4083.644331...\n",
      "Epoch 22/100... Training Loss 3233.489422... Validation Loss: 4060.760349...\n",
      "Starting Epoch 23\n",
      "Epoch 23/100... Training Loss 4502.041384... Validation Loss: 4026.647215...\n",
      "Epoch 23/100... Training Loss 700.404427... Validation Loss: 4005.171866...\n",
      "Epoch 23/100... Training Loss 2974.314139... Validation Loss: 3987.212041...\n",
      "Epoch 23/100... Training Loss 3044.097400... Validation Loss: 3968.466895...\n",
      "Starting Epoch 24\n",
      "Epoch 24/100... Training Loss 4360.330034... Validation Loss: 3939.223071...\n",
      "Epoch 24/100... Training Loss 547.818153... Validation Loss: 3921.345159...\n",
      "Epoch 24/100... Training Loss 2439.450637... Validation Loss: 3908.835328...\n",
      "Epoch 24/100... Training Loss 2590.216857... Validation Loss: 3894.515075...\n",
      "Starting Epoch 25\n",
      "Epoch 25/100... Training Loss 4225.444986... Validation Loss: 3870.426230...\n",
      "Epoch 25/100... Training Loss 512.354938... Validation Loss: 3855.206363...\n",
      "Epoch 25/100... Training Loss 2700.850413... Validation Loss: 3844.826445...\n",
      "Epoch 25/100... Training Loss 2383.059058... Validation Loss: 3832.275130...\n",
      "Starting Epoch 26\n",
      "Epoch 26/100... Training Loss 4448.413763... Validation Loss: 3810.275873...\n",
      "Epoch 26/100... Training Loss 552.987576... Validation Loss: 3796.149263...\n",
      "Epoch 26/100... Training Loss 2562.898774... Validation Loss: 3786.653337...\n",
      "Epoch 26/100... Training Loss 2235.362781... Validation Loss: 3774.848778...\n",
      "Starting Epoch 27\n",
      "Epoch 27/100... Training Loss 4026.790478... Validation Loss: 3753.796103...\n",
      "Epoch 27/100... Training Loss 594.682205... Validation Loss: 3740.186838...\n",
      "Epoch 27/100... Training Loss 2862.213281... Validation Loss: 3731.076328...\n",
      "Epoch 27/100... Training Loss 2528.174509... Validation Loss: 3719.631083...\n",
      "Starting Epoch 28\n",
      "Epoch 28/100... Training Loss 4311.066761... Validation Loss: 3699.047224...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100... Training Loss 560.381871... Validation Loss: 3685.698280...\n",
      "Epoch 28/100... Training Loss 2908.421042... Validation Loss: 3676.807983...\n",
      "Epoch 28/100... Training Loss 2423.326747... Validation Loss: 3665.567279...\n",
      "Starting Epoch 29\n",
      "Epoch 29/100... Training Loss 3588.804933... Validation Loss: 3645.293631...\n",
      "Epoch 29/100... Training Loss 640.803401... Validation Loss: 3632.121348...\n",
      "Epoch 29/100... Training Loss 3397.900979... Validation Loss: 3623.358208...\n",
      "Epoch 29/100... Training Loss 2509.823108... Validation Loss: 3612.257711...\n",
      "Starting Epoch 30\n",
      "Epoch 30/100... Training Loss 4388.321173... Validation Loss: 3592.204839...\n",
      "Epoch 30/100... Training Loss 494.816177... Validation Loss: 3579.176649...\n",
      "Epoch 30/100... Training Loss 2691.998752... Validation Loss: 3570.517176...\n",
      "Epoch 30/100... Training Loss 1833.520078... Validation Loss: 3559.533682...\n",
      "Starting Epoch 31\n",
      "Epoch 31/100... Training Loss 4045.209923... Validation Loss: 3539.673387...\n",
      "Epoch 31/100... Training Loss 500.309673... Validation Loss: 3526.767602...\n",
      "Epoch 31/100... Training Loss 2359.894292... Validation Loss: 3518.194637...\n",
      "Epoch 31/100... Training Loss 2163.587501... Validation Loss: 3507.320540...\n",
      "Starting Epoch 32\n",
      "Epoch 32/100... Training Loss 4700.221283... Validation Loss: 3487.656408...\n",
      "Epoch 32/100... Training Loss 545.181103... Validation Loss: 3474.874953...\n",
      "Epoch 32/100... Training Loss 2479.027032... Validation Loss: 3466.384730...\n",
      "Epoch 32/100... Training Loss 1880.466068... Validation Loss: 3455.612118...\n",
      "Starting Epoch 33\n",
      "Epoch 33/100... Training Loss 3948.128960... Validation Loss: 3436.122351...\n",
      "Epoch 33/100... Training Loss 625.172137... Validation Loss: 3423.456083...\n",
      "Epoch 33/100... Training Loss 2431.474219... Validation Loss: 3415.044548...\n",
      "Epoch 33/100... Training Loss 2267.092579... Validation Loss: 3404.370855...\n",
      "Starting Epoch 34\n",
      "Epoch 34/100... Training Loss 4286.215077... Validation Loss: 3385.067114...\n",
      "Epoch 34/100... Training Loss 747.681673... Validation Loss: 3372.519719...\n",
      "Epoch 34/100... Training Loss 2113.307842... Validation Loss: 3364.187322...\n",
      "Epoch 34/100... Training Loss 2834.716749... Validation Loss: 3353.607153...\n",
      "Starting Epoch 35\n",
      "Epoch 35/100... Training Loss 3925.772411... Validation Loss: 3334.474456...\n",
      "Epoch 35/100... Training Loss 536.779618... Validation Loss: 3322.039318...\n",
      "Epoch 35/100... Training Loss 2323.259512... Validation Loss: 3313.783823...\n",
      "Epoch 35/100... Training Loss 2213.422269... Validation Loss: 3303.302227...\n",
      "Starting Epoch 36\n",
      "Epoch 36/100... Training Loss 4637.667107... Validation Loss: 3284.335960...\n",
      "Epoch 36/100... Training Loss 541.223730... Validation Loss: 3272.008471...\n",
      "Epoch 36/100... Training Loss 1914.068588... Validation Loss: 3263.824047...\n",
      "Epoch 36/100... Training Loss 2592.322248... Validation Loss: 3253.438809...\n",
      "Starting Epoch 37\n",
      "Epoch 37/100... Training Loss 4182.613298... Validation Loss: 3234.645919...\n",
      "Epoch 37/100... Training Loss 519.227709... Validation Loss: 3222.433709...\n",
      "Epoch 37/100... Training Loss 2527.166276... Validation Loss: 3214.328394...\n",
      "Epoch 37/100... Training Loss 2157.338986... Validation Loss: 3204.040790...\n",
      "Starting Epoch 38\n",
      "Epoch 38/100... Training Loss 3558.025533... Validation Loss: 3185.424619...\n",
      "Epoch 38/100... Training Loss 565.964360... Validation Loss: 3173.327774...\n",
      "Epoch 38/100... Training Loss 2130.876756... Validation Loss: 3165.296627...\n",
      "Epoch 38/100... Training Loss 2259.663792... Validation Loss: 3155.100259...\n",
      "Starting Epoch 39\n",
      "Epoch 39/100... Training Loss 3741.877253... Validation Loss: 3136.648744...\n",
      "Epoch 39/100... Training Loss 713.796839... Validation Loss: 3124.657142...\n",
      "Epoch 39/100... Training Loss 1939.033990... Validation Loss: 3116.697193...\n",
      "Epoch 39/100... Training Loss 2066.138474... Validation Loss: 3106.592292...\n",
      "Starting Epoch 40\n",
      "Epoch 40/100... Training Loss 3339.037604... Validation Loss: 3088.311906...\n",
      "Epoch 40/100... Training Loss 683.102083... Validation Loss: 3076.432362...\n",
      "Epoch 40/100... Training Loss 2703.098333... Validation Loss: 3068.547429...\n",
      "Epoch 40/100... Training Loss 2238.457007... Validation Loss: 3058.534208...\n",
      "Starting Epoch 41\n",
      "Epoch 41/100... Training Loss 3773.751221... Validation Loss: 3040.425675...\n",
      "Epoch 41/100... Training Loss 728.058219... Validation Loss: 3028.656968...\n",
      "Epoch 41/100... Training Loss 1575.917587... Validation Loss: 3020.848115...\n",
      "Epoch 41/100... Training Loss 2216.357894... Validation Loss: 3010.931107...\n",
      "Starting Epoch 42\n",
      "Epoch 42/100... Training Loss 3411.353728... Validation Loss: 2992.987806...\n",
      "Epoch 42/100... Training Loss 731.772874... Validation Loss: 2981.332424...\n",
      "Epoch 42/100... Training Loss 1768.200823... Validation Loss: 2973.596914...\n",
      "Epoch 42/100... Training Loss 2463.030823... Validation Loss: 2963.772588...\n",
      "Starting Epoch 43\n",
      "Epoch 43/100... Training Loss 4263.419362... Validation Loss: 2945.999414...\n",
      "Epoch 43/100... Training Loss 656.472646... Validation Loss: 2934.452784...\n",
      "Epoch 43/100... Training Loss 2805.940984... Validation Loss: 2926.790970...\n",
      "Epoch 43/100... Training Loss 2029.147550... Validation Loss: 2917.062006...\n",
      "Starting Epoch 44\n",
      "Epoch 44/100... Training Loss 4087.635936... Validation Loss: 2899.456047...\n",
      "Epoch 44/100... Training Loss 612.995775... Validation Loss: 2888.015840...\n",
      "Epoch 44/100... Training Loss 2429.467941... Validation Loss: 2880.427026...\n",
      "Epoch 44/100... Training Loss 2145.980148... Validation Loss: 2870.788528...\n",
      "Starting Epoch 45\n",
      "Epoch 45/100... Training Loss 4308.756218... Validation Loss: 2853.348719...\n",
      "Epoch 45/100... Training Loss 672.058375... Validation Loss: 2842.013733...\n",
      "Epoch 45/100... Training Loss 1652.596986... Validation Loss: 2834.491098...\n",
      "Epoch 45/100... Training Loss 1825.850546... Validation Loss: 2824.938153...\n",
      "Starting Epoch 46\n",
      "Epoch 46/100... Training Loss 3877.125006... Validation Loss: 2807.653575...\n",
      "Epoch 46/100... Training Loss 620.397472... Validation Loss: 2796.422086...\n",
      "Epoch 46/100... Training Loss 2729.128386... Validation Loss: 2788.969101...\n",
      "Epoch 46/100... Training Loss 1501.036193... Validation Loss: 2779.508902...\n",
      "Starting Epoch 47\n",
      "Epoch 47/100... Training Loss 3805.576361... Validation Loss: 2762.394982...\n",
      "Epoch 47/100... Training Loss 863.202786... Validation Loss: 2751.272278...\n",
      "Epoch 47/100... Training Loss 2542.123583... Validation Loss: 2743.892029...\n",
      "Epoch 47/100... Training Loss 1926.108294... Validation Loss: 2734.522463...\n",
      "Starting Epoch 48\n",
      "Epoch 48/100... Training Loss 3198.303499... Validation Loss: 2717.571791...\n",
      "Epoch 48/100... Training Loss 669.631595... Validation Loss: 2706.556854...\n",
      "Epoch 48/100... Training Loss 1567.513095... Validation Loss: 2699.250260...\n",
      "Epoch 48/100... Training Loss 2085.587826... Validation Loss: 2689.974998...\n",
      "Starting Epoch 49\n",
      "Epoch 49/100... Training Loss 3790.126235... Validation Loss: 2673.195305...\n",
      "Epoch 49/100... Training Loss 834.008831... Validation Loss: 2662.291733...\n",
      "Epoch 49/100... Training Loss 2330.200370... Validation Loss: 2655.058593...\n",
      "Epoch 49/100... Training Loss 2049.171271... Validation Loss: 2645.876333...\n",
      "Starting Epoch 50\n",
      "Epoch 50/100... Training Loss 3174.530019... Validation Loss: 2629.261516...\n",
      "Epoch 50/100... Training Loss 738.921538... Validation Loss: 2618.467769...\n",
      "Epoch 50/100... Training Loss 2463.450547... Validation Loss: 2611.308313...\n",
      "Epoch 50/100... Training Loss 2634.556538... Validation Loss: 2602.216546...\n",
      "Starting Epoch 51\n",
      "Epoch 51/100... Training Loss 3293.115234... Validation Loss: 2585.768122...\n",
      "Epoch 51/100... Training Loss 691.691658... Validation Loss: 2575.081623...\n",
      "Epoch 51/100... Training Loss 2333.865959... Validation Loss: 2567.994175...\n",
      "Epoch 51/100... Training Loss 1503.070752... Validation Loss: 2558.994346...\n",
      "Starting Epoch 52\n",
      "Epoch 52/100... Training Loss 3968.260055... Validation Loss: 2542.710483...\n",
      "Epoch 52/100... Training Loss 873.859203... Validation Loss: 2532.129860...\n",
      "Epoch 52/100... Training Loss 2606.430140... Validation Loss: 2525.112671...\n",
      "Epoch 52/100... Training Loss 2459.125629... Validation Loss: 2516.202339...\n",
      "Starting Epoch 53\n",
      "Epoch 53/100... Training Loss 2932.766624... Validation Loss: 2500.078590...\n",
      "Epoch 53/100... Training Loss 806.431765... Validation Loss: 2489.603656...\n",
      "Epoch 53/100... Training Loss 2120.040191... Validation Loss: 2482.653327...\n",
      "Epoch 53/100... Training Loss 1982.017714... Validation Loss: 2473.825869...\n",
      "Starting Epoch 54\n",
      "Epoch 54/100... Training Loss 3878.433804... Validation Loss: 2457.860013...\n",
      "Epoch 54/100... Training Loss 875.505425... Validation Loss: 2447.490173...\n",
      "Epoch 54/100... Training Loss 2246.028694... Validation Loss: 2440.613683...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100... Training Loss 1765.159539... Validation Loss: 2431.881171...\n",
      "Starting Epoch 55\n",
      "Epoch 55/100... Training Loss 1985.259421... Validation Loss: 2416.082826...\n",
      "Epoch 55/100... Training Loss 836.470858... Validation Loss: 2405.820322...\n",
      "Epoch 55/100... Training Loss 1435.996504... Validation Loss: 2399.014822...\n",
      "Epoch 55/100... Training Loss 1784.622868... Validation Loss: 2390.370438...\n",
      "Starting Epoch 56\n",
      "Epoch 56/100... Training Loss 2769.925778... Validation Loss: 2374.734892...\n",
      "Epoch 56/100... Training Loss 945.584178... Validation Loss: 2364.577592...\n",
      "Epoch 56/100... Training Loss 1563.134806... Validation Loss: 2357.840727...\n",
      "Epoch 56/100... Training Loss 1575.426405... Validation Loss: 2349.284956...\n",
      "Starting Epoch 57\n",
      "Epoch 57/100... Training Loss 2962.029453... Validation Loss: 2333.807462...\n",
      "Epoch 57/100... Training Loss 867.989271... Validation Loss: 2323.754700...\n",
      "Epoch 57/100... Training Loss 1694.273448... Validation Loss: 2317.088358...\n",
      "Epoch 57/100... Training Loss 1908.301494... Validation Loss: 2308.624039...\n",
      "Starting Epoch 58\n",
      "Epoch 58/100... Training Loss 3324.519567... Validation Loss: 2293.315902...\n",
      "Epoch 58/100... Training Loss 915.765222... Validation Loss: 2283.373971...\n",
      "Epoch 58/100... Training Loss 1893.462159... Validation Loss: 2276.780271...\n",
      "Epoch 58/100... Training Loss 1743.235145... Validation Loss: 2268.405987...\n",
      "Starting Epoch 59\n",
      "Epoch 59/100... Training Loss 3590.152238... Validation Loss: 2253.256700...\n",
      "Epoch 59/100... Training Loss 921.560242... Validation Loss: 2243.418426...\n",
      "Epoch 59/100... Training Loss 1825.985271... Validation Loss: 2236.894376...\n",
      "Epoch 59/100... Training Loss 2037.511985... Validation Loss: 2228.609516...\n",
      "Starting Epoch 60\n",
      "Epoch 60/100... Training Loss 3493.306548... Validation Loss: 2213.623880...\n",
      "Epoch 60/100... Training Loss 960.226488... Validation Loss: 2203.892425...\n",
      "Epoch 60/100... Training Loss 1841.857811... Validation Loss: 2197.440855...\n",
      "Epoch 60/100... Training Loss 1706.107029... Validation Loss: 2189.245535...\n",
      "Starting Epoch 61\n",
      "Epoch 61/100... Training Loss 2893.271398... Validation Loss: 2174.416450...\n",
      "Epoch 61/100... Training Loss 1049.656192... Validation Loss: 2164.787218...\n",
      "Epoch 61/100... Training Loss 2520.816263... Validation Loss: 2158.402665...\n",
      "Epoch 61/100... Training Loss 1551.201921... Validation Loss: 2150.295063...\n",
      "Starting Epoch 62\n",
      "Epoch 62/100... Training Loss 3270.985254... Validation Loss: 2135.627933...\n",
      "Epoch 62/100... Training Loss 1044.648860... Validation Loss: 2127.959978...\n",
      "Epoch 62/100... Training Loss 1472.974677... Validation Loss: 2123.144346...\n",
      "Epoch 62/100... Training Loss 1709.370804... Validation Loss: 2116.330094...\n",
      "Starting Epoch 63\n",
      "Epoch 63/100... Training Loss 2559.631706... Validation Loss: 2103.196532...\n",
      "Epoch 63/100... Training Loss 1188.017457... Validation Loss: 2094.479903...\n",
      "Epoch 63/100... Training Loss 1954.256968... Validation Loss: 2088.805641...\n",
      "Epoch 63/100... Training Loss 1688.702475... Validation Loss: 2081.328177...\n",
      "Starting Epoch 64\n",
      "Epoch 64/100... Training Loss 2561.257426... Validation Loss: 2067.487070...\n",
      "Epoch 64/100... Training Loss 1051.040126... Validation Loss: 2058.429452...\n",
      "Epoch 64/100... Training Loss 2690.375242... Validation Loss: 2052.465421...\n",
      "Epoch 64/100... Training Loss 2298.158336... Validation Loss: 2044.782831...\n",
      "Starting Epoch 65\n",
      "Epoch 65/100... Training Loss 2512.920231... Validation Loss: 2030.764377...\n",
      "Epoch 65/100... Training Loss 1035.977774... Validation Loss: 2021.684319...\n",
      "Epoch 65/100... Training Loss 1591.391634... Validation Loss: 2015.688455...\n",
      "Epoch 65/100... Training Loss 2313.654743... Validation Loss: 2008.012672...\n",
      "Starting Epoch 66\n",
      "Epoch 66/100... Training Loss 2669.612775... Validation Loss: 1994.061207...\n",
      "Epoch 66/100... Training Loss 1178.046074... Validation Loss: 1987.156694...\n",
      "Epoch 66/100... Training Loss 1417.403587... Validation Loss: 1982.903367...\n",
      "Epoch 66/100... Training Loss 978.530256... Validation Loss: 1976.662200...\n",
      "Starting Epoch 67\n",
      "Epoch 67/100... Training Loss 3761.811734... Validation Loss: 1964.403509...\n",
      "Epoch 67/100... Training Loss 1119.731171... Validation Loss: 1958.952495...\n",
      "Epoch 67/100... Training Loss 2123.864836... Validation Loss: 1955.862050...\n",
      "Epoch 67/100... Training Loss 1473.071640... Validation Loss: 1950.583469...\n",
      "Starting Epoch 68\n",
      "Epoch 68/100... Training Loss 3387.787974... Validation Loss: 1939.473824...\n",
      "Epoch 68/100... Training Loss 1243.758982... Validation Loss: 1934.627271...\n",
      "Epoch 68/100... Training Loss 2286.265437... Validation Loss: 1932.014858...\n",
      "Epoch 68/100... Training Loss 1809.568992... Validation Loss: 1927.144499...\n",
      "Starting Epoch 69\n",
      "Epoch 69/100... Training Loss 2726.064941... Validation Loss: 1916.542731...\n",
      "Epoch 69/100... Training Loss 1140.895238... Validation Loss: 1911.953943...\n",
      "Epoch 69/100... Training Loss 1413.037151... Validation Loss: 1909.539100...\n",
      "Epoch 69/100... Training Loss 2297.138491... Validation Loss: 1904.848262...\n",
      "Starting Epoch 70\n",
      "Epoch 70/100... Training Loss 2659.860721... Validation Loss: 1894.488338...\n",
      "Epoch 70/100... Training Loss 1178.103994... Validation Loss: 1888.737484...\n",
      "Epoch 70/100... Training Loss 1584.002367... Validation Loss: 1885.375676...\n",
      "Epoch 70/100... Training Loss 2806.757303... Validation Loss: 1879.939058...\n",
      "Starting Epoch 71\n",
      "Epoch 71/100... Training Loss 2274.901404... Validation Loss: 1868.756772...\n",
      "Epoch 71/100... Training Loss 1205.926267... Validation Loss: 1863.861160...\n",
      "Epoch 71/100... Training Loss 1742.256285... Validation Loss: 1861.179849...\n",
      "Epoch 71/100... Training Loss 2129.335370... Validation Loss: 1856.316080...\n",
      "Starting Epoch 72\n",
      "Epoch 72/100... Training Loss 2903.029722... Validation Loss: 1845.832318...\n",
      "Epoch 72/100... Training Loss 1219.606436... Validation Loss: 1841.285419...\n",
      "Epoch 72/100... Training Loss 2073.836288... Validation Loss: 1838.876050...\n",
      "Epoch 72/100... Training Loss 1290.200705... Validation Loss: 1834.252268...\n",
      "Starting Epoch 73\n",
      "Epoch 73/100... Training Loss 2143.602877... Validation Loss: 1824.080451...\n",
      "Epoch 73/100... Training Loss 1244.055694... Validation Loss: 1819.686791...\n",
      "Epoch 73/100... Training Loss 2598.272572... Validation Loss: 1817.392077...\n",
      "Epoch 73/100... Training Loss 2114.350737... Validation Loss: 1812.880789...\n",
      "Starting Epoch 74\n",
      "Epoch 74/100... Training Loss 2620.328094... Validation Loss: 1802.877016...\n",
      "Epoch 74/100... Training Loss 1275.431475... Validation Loss: 1798.564693...\n",
      "Epoch 74/100... Training Loss 2250.231292... Validation Loss: 1796.326121...\n",
      "Epoch 74/100... Training Loss 2463.547502... Validation Loss: 1791.879140...\n",
      "Starting Epoch 75\n",
      "Epoch 75/100... Training Loss 2167.566933... Validation Loss: 1781.983640...\n",
      "Epoch 75/100... Training Loss 1363.316048... Validation Loss: 1777.719142...\n",
      "Epoch 75/100... Training Loss 1028.064447... Validation Loss: 1775.510199...\n",
      "Epoch 75/100... Training Loss 2145.690767... Validation Loss: 1771.103764...\n",
      "Starting Epoch 76\n",
      "Epoch 76/100... Training Loss 2840.619732... Validation Loss: 1761.289497...\n",
      "Epoch 76/100... Training Loss 1267.921968... Validation Loss: 1757.061829...\n",
      "Epoch 76/100... Training Loss 2091.651591... Validation Loss: 1754.873462...\n",
      "Epoch 76/100... Training Loss 1448.606275... Validation Loss: 1750.507420...\n",
      "Starting Epoch 77\n",
      "Epoch 77/100... Training Loss 2306.051628... Validation Loss: 1740.774535...\n",
      "Epoch 77/100... Training Loss 1328.953902... Validation Loss: 1736.583398...\n",
      "Epoch 77/100... Training Loss 2427.248129... Validation Loss: 1734.416082...\n",
      "Epoch 77/100... Training Loss 1936.683205... Validation Loss: 1730.086419...\n",
      "Starting Epoch 78\n",
      "Epoch 78/100... Training Loss 3313.836802... Validation Loss: 1720.431710...\n",
      "Epoch 78/100... Training Loss 1413.354563... Validation Loss: 1716.274465...\n",
      "Epoch 78/100... Training Loss 2063.695498... Validation Loss: 1714.125540...\n",
      "Epoch 78/100... Training Loss 759.041108... Validation Loss: 1709.830001...\n",
      "Starting Epoch 79\n",
      "Epoch 79/100... Training Loss 1877.270096... Validation Loss: 1700.252059...\n",
      "Epoch 79/100... Training Loss 1373.631072... Validation Loss: 1696.128743...\n",
      "Epoch 79/100... Training Loss 1691.422324... Validation Loss: 1693.998125...\n",
      "Epoch 79/100... Training Loss 1769.699534... Validation Loss: 1689.737067...\n",
      "Starting Epoch 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100... Training Loss 2983.790328... Validation Loss: 1680.234152...\n",
      "Epoch 80/100... Training Loss 1428.426466... Validation Loss: 1676.142938...\n",
      "Epoch 80/100... Training Loss 2598.819822... Validation Loss: 1674.029099...\n",
      "Epoch 80/100... Training Loss 1782.076898... Validation Loss: 1669.801919...\n",
      "Starting Epoch 81\n",
      "Epoch 81/100... Training Loss 1836.453770... Validation Loss: 1660.372830...\n",
      "Epoch 81/100... Training Loss 1346.716860... Validation Loss: 1656.313835...\n",
      "Epoch 81/100... Training Loss 2399.644506... Validation Loss: 1654.217253...\n",
      "Epoch 81/100... Training Loss 1931.804540... Validation Loss: 1650.029387...\n",
      "Starting Epoch 82\n",
      "Epoch 82/100... Training Loss 2996.501510... Validation Loss: 1640.684613...\n",
      "Epoch 82/100... Training Loss 1440.931801... Validation Loss: 1636.662771...\n",
      "Epoch 82/100... Training Loss 630.584043... Validation Loss: 1634.587481...\n",
      "Epoch 82/100... Training Loss 1100.071802... Validation Loss: 1630.433326...\n",
      "Starting Epoch 83\n",
      "Epoch 83/100... Training Loss 2794.999747... Validation Loss: 1621.159763...\n",
      "Epoch 83/100... Training Loss 1451.888800... Validation Loss: 1617.168879...\n",
      "Epoch 83/100... Training Loss 2060.275252... Validation Loss: 1615.109271...\n",
      "Epoch 83/100... Training Loss 1601.294421... Validation Loss: 1610.993888...\n",
      "Starting Epoch 84\n",
      "Epoch 84/100... Training Loss 2288.037513... Validation Loss: 1601.801052...\n",
      "Epoch 84/100... Training Loss 1534.848035... Validation Loss: 1597.846053...\n",
      "Epoch 84/100... Training Loss 2586.030752... Validation Loss: 1595.806489...\n",
      "Epoch 84/100... Training Loss 1936.281566... Validation Loss: 1593.830547...\n",
      "Starting Epoch 85\n",
      "Epoch 85/100... Training Loss 2560.510106... Validation Loss: 1587.148194...\n",
      "Epoch 85/100... Training Loss 1442.765478... Validation Loss: 1584.466816...\n",
      "Epoch 85/100... Training Loss 1854.296510... Validation Loss: 1583.453241...\n",
      "Epoch 85/100... Training Loss 1935.371169... Validation Loss: 1581.740552...\n",
      "Starting Epoch 86\n",
      "Epoch 86/100... Training Loss 1360.443017... Validation Loss: 1575.390471...\n",
      "Epoch 86/100... Training Loss 1477.636041... Validation Loss: 1572.875874...\n",
      "Epoch 86/100... Training Loss 2395.421083... Validation Loss: 1571.992183...\n",
      "Epoch 86/100... Training Loss 2787.559966... Validation Loss: 1568.869863...\n",
      "Starting Epoch 87\n",
      "Epoch 87/100... Training Loss 3898.159817... Validation Loss: 1560.892564...\n",
      "Epoch 87/100... Training Loss 1506.454482... Validation Loss: 1557.545378...\n",
      "Epoch 87/100... Training Loss 1513.602919... Validation Loss: 1555.981210...\n",
      "Epoch 87/100... Training Loss 1931.534925... Validation Loss: 1553.607970...\n",
      "Starting Epoch 88\n",
      "Epoch 88/100... Training Loss 1783.385380... Validation Loss: 1548.127145...\n",
      "Epoch 88/100... Training Loss 1531.743802... Validation Loss: 1546.047393...\n",
      "Epoch 88/100... Training Loss 1866.898741... Validation Loss: 1545.504637...\n",
      "Epoch 88/100... Training Loss 1768.975864... Validation Loss: 1543.305329...\n",
      "Starting Epoch 89\n",
      "Epoch 89/100... Training Loss 2531.613719... Validation Loss: 1536.806362...\n",
      "Epoch 89/100... Training Loss 1525.566659... Validation Loss: 1534.208397...\n",
      "Epoch 89/100... Training Loss 1853.368452... Validation Loss: 1533.343617...\n",
      "Epoch 89/100... Training Loss 2108.341259... Validation Loss: 1532.383570...\n",
      "Starting Epoch 90\n",
      "Epoch 90/100... Training Loss 2506.898578... Validation Loss: 1528.565632...\n",
      "Epoch 90/100... Training Loss 1514.617062... Validation Loss: 1527.352418...\n",
      "Epoch 90/100... Training Loss 2580.129435... Validation Loss: 1527.544859...\n",
      "Epoch 90/100... Training Loss 1599.558570... Validation Loss: 1529.100038...\n",
      "Starting Epoch 91\n",
      "Epoch 91/100... Training Loss 2051.885297... Validation Loss: 1528.544107...\n",
      "Epoch 91/100... Training Loss 1505.774804... Validation Loss: 1529.686503...\n",
      "Epoch 91/100... Training Loss 1150.847479... Validation Loss: 1533.983896...\n",
      "Epoch 91/100... Training Loss 1941.970032... Validation Loss: 1539.941418...\n",
      "Starting Epoch 92\n",
      "Epoch 92/100... Training Loss 2295.904503... Validation Loss: 1547.982687...\n",
      "Epoch 92/100... Training Loss 1451.577771... Validation Loss: 1557.622957...\n",
      "Epoch 92/100... Training Loss 1688.461528... Validation Loss: 1571.876747...\n",
      "Epoch 92/100... Training Loss 1762.154436... Validation Loss: 1586.926550...\n",
      "Starting Epoch 93\n",
      "Epoch 93/100... Training Loss 1780.013021... Validation Loss: 1594.344072...\n",
      "Epoch 93/100... Training Loss 1424.622174... Validation Loss: 1603.546621...\n",
      "Epoch 93/100... Training Loss 2399.760353... Validation Loss: 1624.494566...\n",
      "Epoch 93/100... Training Loss 1756.949350... Validation Loss: 1661.541535...\n",
      "Starting Epoch 94\n",
      "Epoch 94/100... Training Loss 2530.280269... Validation Loss: 1653.457963...\n",
      "Epoch 94/100... Training Loss 1345.251962... Validation Loss: 1662.674056...\n",
      "Epoch 94/100... Training Loss 2029.676326... Validation Loss: 1686.485896...\n",
      "Epoch 94/100... Training Loss 1918.036209... Validation Loss: 1694.859224...\n",
      "Starting Epoch 95\n",
      "Epoch 95/100... Training Loss 2537.801037... Validation Loss: 1668.887377...\n",
      "Epoch 95/100... Training Loss 1353.248956... Validation Loss: 1669.092113...\n",
      "Epoch 95/100... Training Loss 2562.997648... Validation Loss: 1679.287424...\n",
      "Epoch 95/100... Training Loss 1924.396163... Validation Loss: 1680.786501...\n",
      "Starting Epoch 96\n",
      "Epoch 96/100... Training Loss 2311.646731... Validation Loss: 1663.272508...\n",
      "Epoch 96/100... Training Loss 1316.581415... Validation Loss: 1662.113622...\n",
      "Epoch 96/100... Training Loss 1500.290925... Validation Loss: 1668.406883...\n",
      "Epoch 96/100... Training Loss 1751.466647... Validation Loss: 1669.426439...\n",
      "Starting Epoch 97\n",
      "Epoch 97/100... Training Loss 2775.141402... Validation Loss: 1655.539021...\n",
      "Epoch 97/100... Training Loss 1364.698332... Validation Loss: 1654.486281...\n",
      "Epoch 97/100... Training Loss 1146.759173... Validation Loss: 1660.028187...\n",
      "Epoch 97/100... Training Loss 1921.011818... Validation Loss: 1660.769872...\n",
      "Starting Epoch 98\n",
      "Epoch 98/100... Training Loss 3704.045097... Validation Loss: 1650.241427...\n",
      "Epoch 98/100... Training Loss 1384.901793... Validation Loss: 1649.851172...\n",
      "Epoch 98/100... Training Loss 2022.318489... Validation Loss: 1654.420485...\n",
      "Epoch 98/100... Training Loss 2084.306117... Validation Loss: 1654.348737...\n",
      "Starting Epoch 99\n",
      "Epoch 99/100... Training Loss 2992.234918... Validation Loss: 1643.515487...\n",
      "Epoch 99/100... Training Loss 1396.368638... Validation Loss: 1641.921372...\n",
      "Epoch 99/100... Training Loss 1845.150714... Validation Loss: 1644.883594...\n",
      "Epoch 99/100... Training Loss 1572.818272... Validation Loss: 1643.795465...\n",
      "Starting Epoch 100\n",
      "Epoch 100/100... Training Loss 2277.622451... Validation Loss: 1632.441288...\n",
      "Epoch 100/100... Training Loss 1403.662585... Validation Loss: 1630.393873...\n",
      "Epoch 100/100... Training Loss 1670.835792... Validation Loss: 1632.736404...\n",
      "Epoch 100/100... Training Loss 1574.086345... Validation Loss: 1631.586725...\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "batch_size = 10\n",
    "learning_rate = 0.003\n",
    "seq_length = days_in_sequence\n",
    "clip = 5\n",
    "input_length = 6\n",
    "\n",
    "print_every = 2\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "training_losses = [x for x in range(epochs)]\n",
    "validation_losses = [x for x in range(epochs)]\n",
    "\n",
    "# Set to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting Epoch {}'.format(epoch+1))\n",
    "    batches_processed = 0\n",
    "    \n",
    "    # Get batch data\n",
    "    for batch, labels in dataloader(train_data, train_labels,\n",
    "                                                input_features=input_length,\n",
    "                                                days_in_sequence=seq_length,\n",
    "                                                batch_size=batch_size):\n",
    "        # Increment step count\n",
    "        batches_processed += 1\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        train_hidden = model.init_hidden(batch_size)\n",
    "        train_hidden = tuple([each.data for each in train_hidden])\n",
    "        \n",
    "        # Set tensors to correct device\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        for each in train_hidden:\n",
    "            each.to(device)\n",
    "            \n",
    "        # Zero out gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Run batch data through model\n",
    "        train_out, train_hidden = model(batch, train_hidden)\n",
    "        \n",
    "        # Calculate loss and perform back propogation -- clip gradients if necessary\n",
    "        loss = criterion(train_out, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Take optimizer step to update model weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation #\n",
    "        if batches_processed % print_every == 0:\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            \n",
    "            # Iterate through test data to validate model performance\n",
    "            for val_batch, val_labels in dataloader(test_data, test_labels,\n",
    "                                                    input_features=input_length,\n",
    "                                                    days_in_sequence=seq_length,\n",
    "                                                    batch_size=batch_size):\n",
    "                # Initialize hidden state\n",
    "                val_hidden = model.init_hidden(batch_size)\n",
    "                val_hidden = tuple([each.data for each in val_hidden])\n",
    "                \n",
    "                # Set tensors to correct device\n",
    "                val_batch, val_labels = val_batch.to(device), val_labels.to(device)\n",
    "                for each in val_hidden:\n",
    "                    each.to(device)\n",
    "                    \n",
    "                # Run data through network\n",
    "                val_output, val_hidden = model(val_batch, val_hidden)\n",
    "                \n",
    "                # Calculate loss\n",
    "                val_loss = criterion(val_output, val_labels)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            # Print out metrics\n",
    "            print('Epoch {}/{}...'.format(epoch+1, epochs),\n",
    "                  'Training Loss {:.6f}...'.format(loss.item()),\n",
    "                  'Validation Loss: {:.6f}...'.format(np.mean(val_losses)))\n",
    "            \n",
    "            # Record metrics\n",
    "            training_losses[epoch] = loss.item()\n",
    "            validation_losses[epoch] = np.mean(val_losses)\n",
    "            \n",
    "            # Set model back to train\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Training/Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:julie-stav-ws]",
   "language": "python",
   "name": "conda-env-julie-stav-ws-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
