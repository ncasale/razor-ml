{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse DRF Files to get Relevant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_master_df(path, num_races):\n",
    "    '''\n",
    "        Generate the master dataframe from which we will create our training/testing data\n",
    "        \n",
    "        Args:\n",
    "            path (string): Path to directory containing DRF files to parse\n",
    "            num_races (int): Number of races to use in each sequence (how many races back\n",
    "                             are we looking?)\n",
    "        \n",
    "        Returns: Dataframe containing all data from each DRF concatted together\n",
    "    '''\n",
    "    # Cap num_races\n",
    "    num_races = min(num_races, 9) # Only have max of 9 prev race's data\n",
    "    \n",
    "    # Get all DRF files in data directory\n",
    "    filenames = [path+file for file in os.listdir(path) if file.endswith(\".DRF\")]\n",
    "    \n",
    "    # Iterate through each file and concat data to master df\n",
    "    master_df = None\n",
    "    for ii, file in enumerate(filenames): \n",
    "        if ii == 0:\n",
    "            # First pass through just create master df\n",
    "            df = pd.read_csv(file, header=None)\n",
    "            master_df = slice_df(df, num_races)\n",
    "        else:\n",
    "            # All other passes, append sliced dataframe to master\n",
    "            df = pd.read_csv(file, header=None)\n",
    "            df = slice_df(df, num_races)\n",
    "            master_df = master_df.append(df, ignore_index=True)\n",
    "            \n",
    "    # Drop all rows containing NaN values (these horses didn't have enough prev races)\n",
    "    return master_df.dropna().reset_index().drop(['index'], axis=1)\n",
    "\n",
    "def slice_df(df, num_races=3):\n",
    "    # Define columns to grab\n",
    "    column_ids = OrderedDict({\n",
    "        'horse_age': (46,47),\n",
    "        'days_since_prev_race': (266, 266+num_races),\n",
    "        'distance': (316, 316+num_races),\n",
    "        'num_entrants': (346, 346+num_races),\n",
    "        'post_position': (356, 356+num_races),\n",
    "        'weight': (506, 506+num_races),\n",
    "        'label': (1036, 1036+num_races) # Finish time\n",
    "    })\n",
    "\n",
    "    # Select all of our column ranges\n",
    "    rng = []\n",
    "    col_names = []\n",
    "    for k,v in column_ids.items():\n",
    "        # Append range to rng -- special case for single field\n",
    "        if v[1] - v[0] == 1:\n",
    "            for i in range(num_races):\n",
    "                rng += [v[0]]\n",
    "                col_names.append('{}_{}'.format(k, i))\n",
    "        else:\n",
    "            # Handle column ranges\n",
    "            rng += range(v[0],v[1])\n",
    "            for ii in range(v[0], v[1]):\n",
    "                col_names.append('{}_{}'.format(k, ii-v[0]))\n",
    "\n",
    "    # Slice df on columns\n",
    "    ret = df.loc[:, rng]\n",
    "    ret.columns = col_names\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_in_sequence = 3\n",
    "master_df = generate_master_df('./input_files/', days_in_sequence)\n",
    "master_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloader\n",
    "Create a generator that can parse through the master dataframe, and create batches of training data. These batches will have the shape (days_in_sequence, batch_size, input_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Section off data by race -- list of tuples (race_num, data)\n",
    "race_data = []\n",
    "for ii in range(days_in_sequence):\n",
    "    # Match all collumns for this race\n",
    "    pattern = re.compile('.*_{}'.format(ii))\n",
    "    cols = [pattern.match(col).string for col in master_df.columns if pattern.match(col) != None]\n",
    "    # Get data from these columns\n",
    "    data = master_df.loc[:, cols]\n",
    "    # Rename columns\n",
    "    cols = [col[:-2] for col in cols]\n",
    "    data.columns = cols\n",
    "    # Append to race data\n",
    "    race_data.append((ii, data)) \n",
    "    \n",
    "# Break race_data into input_data and label_data\n",
    "input_data = []\n",
    "labels = []\n",
    "for race_tup in race_data:\n",
    "    input_data.append(race_tup[1].drop(['label'], axis=1).values)\n",
    "    labels.append(race_tup[1]['label'].values)\n",
    "    \n",
    "# Want data to go in reverse order (oldest races first)\n",
    "input_data.reverse()\n",
    "labels.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(data, labels, days_in_sequence=3, batch_size=10, input_features=6):\n",
    "    # Truncate data to ensure only full batches\n",
    "    num_horses = len(data[0])\n",
    "    cutoff = (num_horses//batch_size)*batch_size\n",
    "    trunc_data = [race[:cutoff] for race in data]\n",
    "    trunc_labels = [race[:cutoff] for race in labels]\n",
    "    \n",
    "    # Create our batches\n",
    "    for ii in range(0, cutoff, batch_size):\n",
    "        # Get data for this batch\n",
    "        batch_data = [race[ii:ii+batch_size] for race in trunc_data]\n",
    "        batch_labels = [race[ii: ii+batch_size] for race in trunc_labels]\n",
    "        \n",
    "        # Create batch tensor of correct size -- days_in_sequence X batch_size X input_features\n",
    "        batch = torch.zeros((days_in_sequence, batch_size, input_features), dtype=torch.float64)\n",
    "        \n",
    "        # Fill in batch tensor\n",
    "        for batch_col in range(0, batch_size):\n",
    "            # Create sequence -- grab horse data from each race -- and add to batch\n",
    "            sequence = torch.tensor([batch_data[i][batch_col] for i in range(0, days_in_sequence)])\n",
    "            batch[:, batch_col] = sequence\n",
    "            \n",
    "        # Create label tensor\n",
    "        label_tensor = torch.tensor(batch_labels[-1], dtype=torch.float64)\n",
    "        \n",
    "        yield batch, label_tensor\n",
    "    \n",
    "    \n",
    "test = dataloader(input_data, labels)\n",
    "\n",
    "sample_batch, sample_label = next(iter(dataloader(input_data, labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandicappingBrain(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_length=6,lstm_size=64, lstm_layers=1, output_size=1, \n",
    "                               drop_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.input_length = input_length\n",
    "        self.output_size = output_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        ## LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_length, lstm_size, lstm_layers, \n",
    "                            dropout=drop_prob, batch_first=False)\n",
    "        \n",
    "        ## Dropout Layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## Fully-connected Output Layer\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "      \n",
    "    \n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        '''\n",
    "            Perform a forward pass through the network\n",
    "            \n",
    "            Args:\n",
    "                nn_input: the batch of input to NN\n",
    "                hidden_state: The LSTM hidden/cell state tuple\n",
    "                \n",
    "            Returns:\n",
    "                logps: log softmax output\n",
    "                hidden_state: the updated hidden/cell state tuple\n",
    "        '''\n",
    "        # Input -> LSTM\n",
    "        lstm_out, hidden_state = self.lstm(nn_input, hidden_state)\n",
    "\n",
    "        # Stack up LSTM outputs -- this gets the final LSTM output for each sequence in the batch\n",
    "        lstm_out = lstm_out[-1, :, :]\n",
    "        \n",
    "        # LSTM -> Dense Layer\n",
    "        dense_out = self.dropout(self.fc(lstm_out))\n",
    "        \n",
    "        # Return the final output and the hidden state\n",
    "        return dense_out, hidden_state\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "              weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = HandicappingBrain(input_length=6, lstm_size=8, lstm_layers=1, drop_prob=0.2, output_size=1).double()\n",
    "hidden = test_model.init_hidden(10)\n",
    "dense_out, _ = test_model.forward(sample_batch, hidden)\n",
    "print(dense_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Test/Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prop = 0.2\n",
    "test_end_idx = int(len(input_data[0]) * test_prop)\n",
    "\n",
    "# Create test set -- test_prob% of our total data set\n",
    "test_data = [race[:test_end_idx] for race in input_data]\n",
    "test_labels = [race[:test_end_idx] for race in labels]\n",
    "\n",
    "# Craete training set\n",
    "train_data = [race[test_end_idx:] for race in input_data]\n",
    "train_labels = [race[test_end_idx:] for race in labels]\n",
    "\n",
    "print(len(train_data[0]), len(train_labels[0]))\n",
    "print(len(test_data[0]), len(test_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on GPU if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define model -- set dtype to double since our data requires it\n",
    "model = HandicappingBrain(input_length=6, lstm_size=32, lstm_layers=2, output_size=1, drop_prob=0.3).double()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 10\n",
    "learning_rate = 0.003\n",
    "seq_length = days_in_sequence\n",
    "clip = 5\n",
    "input_length = 6\n",
    "\n",
    "print_every = 2\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "training_losses = [x for x in range(epochs)]\n",
    "validation_losses = [x for x in range(epochs)]\n",
    "\n",
    "# Set to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting Epoch {}'.format(epoch+1))\n",
    "    batches_processed = 0\n",
    "    \n",
    "    # Get batch data\n",
    "    for batch, labels in dataloader(train_data, train_labels,\n",
    "                                                input_features=input_length,\n",
    "                                                days_in_sequence=seq_length,\n",
    "                                                batch_size=batch_size):\n",
    "        # Increment step count\n",
    "        batches_processed += 1\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        train_hidden = model.init_hidden(batch_size)\n",
    "        train_hidden = tuple([each.data for each in train_hidden])\n",
    "        \n",
    "        # Set tensors to correct device\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        for each in train_hidden:\n",
    "            each.to(device)\n",
    "            \n",
    "        # Zero out gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Run batch data through model\n",
    "        train_out, train_hidden = model(batch, train_hidden)\n",
    "        \n",
    "        # Calculate loss and perform back propogation -- clip gradients if necessary\n",
    "        loss = criterion(train_out, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Take optimizer step to update model weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation #\n",
    "        if batches_processed % print_every == 0:\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            \n",
    "            # Iterate through test data to validate model performance\n",
    "            for val_batch, val_labels in dataloader(test_data, test_labels,\n",
    "                                                    input_features=input_length,\n",
    "                                                    days_in_sequence=seq_length,\n",
    "                                                    batch_size=batch_size):\n",
    "                # Initialize hidden state\n",
    "                val_hidden = model.init_hidden(batch_size)\n",
    "                val_hidden = tuple([each.data for each in val_hidden])\n",
    "                \n",
    "                # Set tensors to correct device\n",
    "                val_batch, val_labels = val_batch.to(device), val_labels.to(device)\n",
    "                for each in val_hidden:\n",
    "                    each.to(device)\n",
    "                    \n",
    "                # Run data through network\n",
    "                val_output, val_hidden = model(val_batch, val_hidden)\n",
    "                \n",
    "                # Calculate loss\n",
    "                val_loss = criterion(val_output, val_labels)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            # Print out metrics\n",
    "            print('Epoch {}/{}...'.format(epoch+1, epochs),\n",
    "                  'Training Loss {:.6f}...'.format(loss.item()),\n",
    "                  'Validation Loss: {:.6f}...'.format(np.mean(val_losses)))\n",
    "            \n",
    "            # Record metrics\n",
    "            training_losses[epoch] = loss.item()\n",
    "            validation_losses[epoch] = np.mean(val_losses)\n",
    "            \n",
    "            # Set model back to train\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Training/Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:julie-stav-ws]",
   "language": "python",
   "name": "conda-env-julie-stav-ws-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
