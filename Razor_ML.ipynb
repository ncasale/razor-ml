{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse DRF Files to get Relevant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_master_df(path, num_races):\n",
    "    '''\n",
    "        Generate the master dataframe from which we will create our training/testing data\n",
    "        \n",
    "        Args:\n",
    "            path (string): Path to directory containing DRF files to parse\n",
    "            num_races (int): Number of races to use in each sequence (how many races back\n",
    "                             are we looking?)\n",
    "        \n",
    "        Returns: Dataframe containing all data from each DRF concatted together\n",
    "    '''\n",
    "    # Cap num_races\n",
    "    num_races = min(num_races, 9) # Only have max of 9 prev race's data\n",
    "    \n",
    "    # Get all DRF files in data directory\n",
    "    filenames = [path+file for file in os.listdir(path) if file.endswith(\".DRF\")]\n",
    "    \n",
    "    # Iterate through each file and concat data to master df\n",
    "    master_df = None\n",
    "    for ii, file in enumerate(filenames): \n",
    "        if ii == 0:\n",
    "            # First pass through just create master df\n",
    "            df = pd.read_csv(file, header=None)\n",
    "            master_df = slice_df(df, num_races)\n",
    "        else:\n",
    "            # All other passes, append sliced dataframe to master\n",
    "            df = pd.read_csv(file, header=None)\n",
    "            df = slice_df(df, num_races)\n",
    "            master_df = master_df.append(df, ignore_index=True)\n",
    "            \n",
    "    # Drop all rows containing NaN values (these horses didn't have enough prev races)\n",
    "    return master_df.dropna().reset_index().drop(['index'], axis=1)\n",
    "\n",
    "def slice_df(df, num_races=3):\n",
    "    # Define columns to grab\n",
    "    column_ids = OrderedDict({\n",
    "        'horse_age': (46,47),\n",
    "        'days_since_prev_race': (266, 266+num_races),\n",
    "        'distance': (316, 316+num_races),\n",
    "        'num_entrants': (346, 346+num_races),\n",
    "        'post_position': (356, 356+num_races),\n",
    "        'weight': (506, 506+num_races),\n",
    "        'label': (1036, 1036+num_races) # Finish time\n",
    "    })\n",
    "\n",
    "    # Select all of our column ranges\n",
    "    rng = []\n",
    "    col_names = []\n",
    "    for k,v in column_ids.items():\n",
    "        # Append range to rng -- special case for single field\n",
    "        if v[1] - v[0] == 1:\n",
    "            for i in range(num_races):\n",
    "                rng += [v[0]]\n",
    "                col_names.append('{}_{}'.format(k, i))\n",
    "        else:\n",
    "            # Handle column ranges\n",
    "            rng += range(v[0],v[1])\n",
    "            for ii in range(v[0], v[1]):\n",
    "                col_names.append('{}_{}'.format(k, ii-v[0]))\n",
    "\n",
    "    # Slice df on columns\n",
    "    ret = df.loc[:, rng]\n",
    "    ret.columns = col_names\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_0</th>\n",
       "      <th>weight_1</th>\n",
       "      <th>weight_2</th>\n",
       "      <th>days_since_prev_race_0</th>\n",
       "      <th>days_since_prev_race_1</th>\n",
       "      <th>days_since_prev_race_2</th>\n",
       "      <th>post_position_0</th>\n",
       "      <th>post_position_1</th>\n",
       "      <th>post_position_2</th>\n",
       "      <th>label_0</th>\n",
       "      <th>...</th>\n",
       "      <th>label_2</th>\n",
       "      <th>distance_0</th>\n",
       "      <th>distance_1</th>\n",
       "      <th>distance_2</th>\n",
       "      <th>num_entrants_0</th>\n",
       "      <th>num_entrants_1</th>\n",
       "      <th>num_entrants_2</th>\n",
       "      <th>horse_age_0</th>\n",
       "      <th>horse_age_1</th>\n",
       "      <th>horse_age_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>75.17</td>\n",
       "      <td>...</td>\n",
       "      <td>74.41</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>75.20</td>\n",
       "      <td>...</td>\n",
       "      <td>80.98</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>104.90</td>\n",
       "      <td>...</td>\n",
       "      <td>108.81</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>1830.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>103.83</td>\n",
       "      <td>...</td>\n",
       "      <td>109.50</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>96.75</td>\n",
       "      <td>...</td>\n",
       "      <td>100.58</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>1870.0</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   weight_0  weight_1  weight_2  days_since_prev_race_0  \\\n",
       "0     121.0     121.0     123.0                    26.0   \n",
       "1     124.0     119.0     124.0                    14.0   \n",
       "2     123.0     121.0     120.0                   147.0   \n",
       "3     121.0     121.0     122.0                    23.0   \n",
       "4     123.0     123.0     118.0                    63.0   \n",
       "\n",
       "   days_since_prev_race_1  days_since_prev_race_2  post_position_0  \\\n",
       "0                    80.0                    11.0              3.0   \n",
       "1                    46.0                    29.0              3.0   \n",
       "2                    17.0                    13.0              5.0   \n",
       "3                    89.0                    14.0              4.0   \n",
       "4                    11.0                    20.0              7.0   \n",
       "\n",
       "   post_position_1  post_position_2  label_0     ...       label_2  \\\n",
       "0              1.0              9.0    75.17     ...         74.41   \n",
       "1              5.0              8.0    75.20     ...         80.98   \n",
       "2              1.0              5.0   104.90     ...        108.81   \n",
       "3              2.0              6.0   103.83     ...        109.50   \n",
       "4              5.0              2.0    96.75     ...        100.58   \n",
       "\n",
       "   distance_0  distance_1  distance_2  num_entrants_0  num_entrants_1  \\\n",
       "0      1320.0      1320.0      1320.0             9.0             7.0   \n",
       "1      1320.0      1320.0      1430.0             8.0             8.0   \n",
       "2      1760.0      1320.0      1830.0             7.0             9.0   \n",
       "3      1760.0      1760.0      1760.0             5.0             5.0   \n",
       "4      1760.0      1870.0      1760.0             7.0            12.0   \n",
       "\n",
       "   num_entrants_2  horse_age_0  horse_age_1  horse_age_2  \n",
       "0            10.0            2            2            2  \n",
       "1             8.0            4            4            4  \n",
       "2             8.0            4            4            4  \n",
       "3             7.0            4            4            4  \n",
       "4             8.0            2            2            2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days_in_sequence = 3\n",
    "master_df = generate_master_df('./input_files/', days_in_sequence)\n",
    "master_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloader\n",
    "Create a generator that can parse through the master dataframe, and create batches of training data. These batches will have the shape (days_in_sequence, batch_size, input_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Section off data by race -- list of tuples (race_num, data)\n",
    "race_data = []\n",
    "for ii in range(days_in_sequence):\n",
    "    # Match all collumns for this race\n",
    "    pattern = re.compile('.*_{}'.format(ii))\n",
    "    cols = [pattern.match(col).string for col in master_df.columns if pattern.match(col) != None]\n",
    "    # Get data from these columns\n",
    "    data = master_df.loc[:, cols]\n",
    "    # Rename columns\n",
    "    cols = [col[:-2] for col in cols]\n",
    "    data.columns = cols\n",
    "    # Append to race data\n",
    "    race_data.append((ii, data)) \n",
    "    \n",
    "# Break race_data into input_data and label_data\n",
    "input_data = []\n",
    "labels = []\n",
    "for race_tup in race_data:\n",
    "    input_data.append(race_tup[1].drop(['label'], axis=1).values)\n",
    "    labels.append(race_tup[1]['label'].values)\n",
    "    \n",
    "# Want data to go in reverse order (oldest races first)\n",
    "input_data.reverse()\n",
    "labels.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(data, labels, days_in_sequence=3, batch_size=10, input_features=6):\n",
    "    # Truncate data to ensure only full batches\n",
    "    num_horses = len(data[0])\n",
    "    cutoff = (num_horses//batch_size)*batch_size\n",
    "    trunc_data = [race[:cutoff] for race in data]\n",
    "    trunc_labels = [race[:cutoff] for race in labels]\n",
    "    \n",
    "    # Create our batches\n",
    "    for ii in range(0, cutoff, batch_size):\n",
    "        # Get data for this batch\n",
    "        batch_data = [race[ii:ii+batch_size] for race in trunc_data]\n",
    "        batch_labels = [race[ii: ii+batch_size] for race in trunc_labels]\n",
    "        \n",
    "        # Create batch tensor of correct size -- days_in_sequence X batch_size X input_features\n",
    "        batch = torch.zeros((days_in_sequence, batch_size, input_features), dtype=torch.float64)\n",
    "        \n",
    "        # Fill in batch tensor\n",
    "        for batch_col in range(0, batch_size):\n",
    "            # Create sequence -- grab horse data from each race -- and add to batch\n",
    "            sequence = torch.tensor([batch_data[i][batch_col] for i in range(0, days_in_sequence)])\n",
    "            batch[:, batch_col] = sequence\n",
    "            \n",
    "        # Create label tensor\n",
    "        label_tensor = torch.tensor(batch_labels[-1], dtype=torch.float64)\n",
    "        \n",
    "        yield batch, label_tensor\n",
    "    \n",
    "    \n",
    "test = dataloader(input_data, labels)\n",
    "\n",
    "sample_batch, sample_label = next(iter(dataloader(input_data, labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandicappingBrain(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_length=6,lstm_size=64, lstm_layers=1, output_size=1, \n",
    "                               drop_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.input_length = input_length\n",
    "        self.output_size = output_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        ## LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_length, lstm_size, lstm_layers, \n",
    "                            dropout=drop_prob, batch_first=False)\n",
    "        \n",
    "        ## Dropout Layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## Fully-connected Output Layer\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "      \n",
    "    \n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        '''\n",
    "            Perform a forward pass through the network\n",
    "            \n",
    "            Args:\n",
    "                nn_input: the batch of input to NN\n",
    "                hidden_state: The LSTM hidden/cell state tuple\n",
    "                \n",
    "            Returns:\n",
    "                logps: log softmax output\n",
    "                hidden_state: the updated hidden/cell state tuple\n",
    "        '''\n",
    "        # Input -> LSTM\n",
    "        lstm_out, hidden_state = self.lstm(nn_input, hidden_state)\n",
    "\n",
    "        # Stack up LSTM outputs -- this gets the final LSTM output for each sequence in the batch\n",
    "        lstm_out = lstm_out[-1, :, :]\n",
    "        \n",
    "        # LSTM -> Dense Layer\n",
    "        dense_out = self.dropout(self.fc(lstm_out))\n",
    "        \n",
    "        # Return the final output and the hidden state\n",
    "        return dense_out, hidden_state\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "              weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0000],\n",
      "        [-0.0192],\n",
      "        [ 0.1095],\n",
      "        [-0.0192],\n",
      "        [-0.0192],\n",
      "        [-0.0192],\n",
      "        [-0.0000],\n",
      "        [-0.0192],\n",
      "        [-0.0192],\n",
      "        [-0.0192]], dtype=torch.float64, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nate/anaconda3/envs/julie-stav-ws/lib/python3.5/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "test_model = HandicappingBrain(input_length=6, lstm_size=8, lstm_layers=1, drop_prob=0.2, output_size=1).double()\n",
    "hidden = test_model.init_hidden(10)\n",
    "dense_out, _ = test_model.forward(sample_batch, hidden)\n",
    "print(dense_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Test/Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 94\n",
      "23 23\n"
     ]
    }
   ],
   "source": [
    "test_prop = 0.2\n",
    "test_end_idx = int(len(input_data[0]) * test_prop)\n",
    "\n",
    "# Create test set -- test_prob% of our total data set\n",
    "test_data = [race[:test_end_idx] for race in input_data]\n",
    "test_labels = [race[:test_end_idx] for race in labels]\n",
    "\n",
    "# Craete training set\n",
    "train_data = [race[test_end_idx:] for race in input_data]\n",
    "train_labels = [race[test_end_idx:] for race in labels]\n",
    "\n",
    "print(len(train_data[0]), len(train_labels[0]))\n",
    "print(len(test_data[0]), len(test_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandicappingBrain(\n",
      "  (lstm): LSTM(6, 32, num_layers=2, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Train on GPU if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define model -- set dtype to double since our data requires it\n",
    "model = HandicappingBrain(input_length=6, lstm_size=32, lstm_layers=2, output_size=1, drop_prob=0.3).double()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Epoch 1/5... Training Loss 7903.348111... Validation Loss: 8203.165919...\n",
      "Epoch 1/5... Training Loss 1375.508609... Validation Loss: 8179.090017...\n",
      "Epoch 1/5... Training Loss 5413.394414... Validation Loss: 8154.266067...\n",
      "Epoch 1/5... Training Loss 5834.645699... Validation Loss: 8126.257274...\n",
      "Starting Epoch 2\n",
      "Epoch 2/5... Training Loss 7802.560028... Validation Loss: 8077.146514...\n",
      "Epoch 2/5... Training Loss 1315.878554... Validation Loss: 8039.705170...\n",
      "Epoch 2/5... Training Loss 5335.077276... Validation Loss: 7997.890883...\n",
      "Epoch 2/5... Training Loss 5683.604237... Validation Loss: 7950.029838...\n",
      "Starting Epoch 3\n",
      "Epoch 3/5... Training Loss 7618.071925... Validation Loss: 7874.291242...\n",
      "Epoch 3/5... Training Loss 1209.531061... Validation Loss: 7820.365279...\n",
      "Epoch 3/5... Training Loss 5058.824154... Validation Loss: 7765.745379...\n",
      "Epoch 3/5... Training Loss 5500.105429... Validation Loss: 7710.265724...\n",
      "Starting Epoch 4\n",
      "Epoch 4/5... Training Loss 7520.666211... Validation Loss: 7626.816309...\n",
      "Epoch 4/5... Training Loss 1099.809472... Validation Loss: 7571.633269...\n",
      "Epoch 4/5... Training Loss 4772.287758... Validation Loss: 7517.150063...\n",
      "Epoch 4/5... Training Loss 5392.361819... Validation Loss: 7463.342807...\n",
      "Starting Epoch 5\n",
      "Epoch 5/5... Training Loss 7045.081235... Validation Loss: 7383.950074...\n",
      "Epoch 5/5... Training Loss 1080.873707... Validation Loss: 7332.129079...\n",
      "Epoch 5/5... Training Loss 4732.508175... Validation Loss: 7281.393945...\n",
      "Epoch 5/5... Training Loss 5119.065591... Validation Loss: 7231.834672...\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 10\n",
    "learning_rate = 0.003\n",
    "seq_length = days_in_sequence\n",
    "clip = 5\n",
    "input_length = 6\n",
    "\n",
    "print_every = 2\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "training_losses = [x for x in range(epochs)]\n",
    "validation_losses = [x for x in range(epochs)]\n",
    "\n",
    "# Set to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting Epoch {}'.format(epoch+1))\n",
    "    batches_processed = 0\n",
    "    \n",
    "    # Get batch data\n",
    "    for batch, labels in dataloader(train_data, train_labels,\n",
    "                                                input_features=input_length,\n",
    "                                                days_in_sequence=seq_length,\n",
    "                                                batch_size=batch_size):\n",
    "        # Increment step count\n",
    "        batches_processed += 1\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        train_hidden = model.init_hidden(batch_size)\n",
    "        train_hidden = tuple([each.data for each in train_hidden])\n",
    "        \n",
    "        # Set tensors to correct device\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        for each in train_hidden:\n",
    "            each.to(device)\n",
    "            \n",
    "        # Zero out gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Run batch data through model\n",
    "        train_out, train_hidden = model(batch, train_hidden)\n",
    "        \n",
    "        # Calculate loss and perform back propogation -- clip gradients if necessary\n",
    "        loss = criterion(train_out, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Take optimizer step to update model weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation #\n",
    "        if batches_processed % print_every == 0:\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            \n",
    "            # Iterate through test data to validate model performance\n",
    "            for val_batch, val_labels in dataloader(test_data, test_labels,\n",
    "                                                    input_features=input_length,\n",
    "                                                    days_in_sequence=seq_length,\n",
    "                                                    batch_size=batch_size):\n",
    "                # Initialize hidden state\n",
    "                val_hidden = model.init_hidden(batch_size)\n",
    "                val_hidden = tuple([each.data for each in val_hidden])\n",
    "                \n",
    "                # Set tensors to correct device\n",
    "                val_batch, val_labels = val_batch.to(device), val_labels.to(device)\n",
    "                for each in val_hidden:\n",
    "                    each.to(device)\n",
    "                    \n",
    "                # Run data through network\n",
    "                val_output, val_hidden = model(val_batch, val_hidden)\n",
    "                \n",
    "                # Calculate loss\n",
    "                val_loss = criterion(val_output, val_labels)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            # Print out metrics\n",
    "            print('Epoch {}/{}...'.format(epoch+1, epochs),\n",
    "                  'Training Loss {:.6f}...'.format(loss.item()),\n",
    "                  'Validation Loss: {:.6f}...'.format(np.mean(val_losses)))\n",
    "            \n",
    "            # Record metrics\n",
    "            training_losses[epoch] = loss.item()\n",
    "            validation_losses[epoch] = np.mean(val_losses)\n",
    "            \n",
    "            # Set model back to train\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Training/Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:julie-stav-ws]",
   "language": "python",
   "name": "conda-env-julie-stav-ws-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
