{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse DRF Files to get Relevant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_master_df(path, num_races):\n",
    "    '''\n",
    "        Generate the master dataframe from which we will create our training/testing data\n",
    "        \n",
    "        Args:\n",
    "            path (string): Path to directory containing DRF files to parse\n",
    "            num_races (int): Number of races to use in each sequence (how many races back\n",
    "                             are we looking?)\n",
    "        \n",
    "        Returns: Dataframe containing all data from each DRF concatted together\n",
    "    '''\n",
    "    # Cap num_races\n",
    "    num_races = min(num_races, 9) # Only have max of 9 prev race's data\n",
    "    \n",
    "    # Get all DRF files in data directory\n",
    "    filenames = [path+file for file in os.listdir(path) if file.endswith(\".DRF\")]\n",
    "    \n",
    "    # Iterate through each file and concat data to master df\n",
    "    master_df = None\n",
    "    for ii, file in tqdm(enumerate(filenames)): \n",
    "        if ii == 0:\n",
    "            # First pass through just create master df\n",
    "            df = pd.read_csv(file, header=None)\n",
    "            master_df = slice_df(df, num_races)\n",
    "        else:\n",
    "            # All other passes, append sliced dataframe to master\n",
    "            df = pd.read_csv(file, header=None)\n",
    "            df = slice_df(df, num_races)\n",
    "            master_df = master_df.append(df, ignore_index=True)\n",
    "            \n",
    "    # Drop all rows containing NaN values (these horses didn't have enough prev races)\n",
    "    return master_df.dropna().reset_index().drop(['index'], axis=1)\n",
    "\n",
    "def slice_df(df, num_races=3):\n",
    "    # Define columns to grab\n",
    "    column_ids = OrderedDict({\n",
    "        'horse_age': (46,47),\n",
    "        'days_since_prev_race': (266, 266+num_races),\n",
    "        'distance': (316, 316+num_races),\n",
    "        'num_entrants': (346, 346+num_races),\n",
    "        'post_position': (356, 356+num_races),\n",
    "        'weight': (506, 506+num_races),\n",
    "        'label': (1036, 1036+num_races) # Finish time\n",
    "    })\n",
    "\n",
    "    # Select all of our column ranges\n",
    "    rng = []\n",
    "    col_names = []\n",
    "    for k,v in column_ids.items():\n",
    "        # Append range to rng -- special case for single field\n",
    "        if v[1] - v[0] == 1:\n",
    "            for i in range(num_races):\n",
    "                rng += [v[0]]\n",
    "                col_names.append('{}_{}'.format(k, i))\n",
    "        else:\n",
    "            # Handle column ranges\n",
    "            rng += range(v[0],v[1])\n",
    "            for ii in range(v[0], v[1]):\n",
    "                col_names.append('{}_{}'.format(k, ii-v[0]))\n",
    "\n",
    "    # Slice df on columns\n",
    "    ret = df.loc[:, rng]\n",
    "    ret.columns = col_names\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "419it [00:43,  9.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horse_age_0</th>\n",
       "      <th>horse_age_1</th>\n",
       "      <th>horse_age_2</th>\n",
       "      <th>days_since_prev_race_0</th>\n",
       "      <th>days_since_prev_race_1</th>\n",
       "      <th>days_since_prev_race_2</th>\n",
       "      <th>distance_0</th>\n",
       "      <th>distance_1</th>\n",
       "      <th>distance_2</th>\n",
       "      <th>num_entrants_0</th>\n",
       "      <th>...</th>\n",
       "      <th>num_entrants_2</th>\n",
       "      <th>post_position_0</th>\n",
       "      <th>post_position_1</th>\n",
       "      <th>post_position_2</th>\n",
       "      <th>weight_0</th>\n",
       "      <th>weight_1</th>\n",
       "      <th>weight_2</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>15.79</td>\n",
       "      <td>18.04</td>\n",
       "      <td>17.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>20.22</td>\n",
       "      <td>17.50</td>\n",
       "      <td>20.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>15.56</td>\n",
       "      <td>12.04</td>\n",
       "      <td>15.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>17.12</td>\n",
       "      <td>15.86</td>\n",
       "      <td>17.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>33.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>18.16</td>\n",
       "      <td>15.92</td>\n",
       "      <td>18.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   horse_age_0  horse_age_1  horse_age_2  days_since_prev_race_0  \\\n",
       "0            3            3            3                    42.0   \n",
       "1            3            3            3                     7.0   \n",
       "2            3            3            3                    14.0   \n",
       "3            2            2            2                    14.0   \n",
       "4            3            3            3                    33.0   \n",
       "\n",
       "   days_since_prev_race_1  days_since_prev_race_2  distance_0  distance_1  \\\n",
       "0                     8.0                    11.0       300.0       350.0   \n",
       "1                    14.0                    78.0       400.0       330.0   \n",
       "2                    14.0                     7.0       300.0       220.0   \n",
       "3                     7.0                    14.0       330.0       300.0   \n",
       "4                    16.0                    22.0       350.0       300.0   \n",
       "\n",
       "   distance_2  num_entrants_0  ...  num_entrants_2  post_position_0  \\\n",
       "0       350.0             8.0  ...            10.0              6.0   \n",
       "1       400.0             6.0  ...             7.0              2.0   \n",
       "2       300.0             8.0  ...             9.0              1.0   \n",
       "3       330.0             7.0  ...             7.0              3.0   \n",
       "4       350.0             9.0  ...             8.0              7.0   \n",
       "\n",
       "   post_position_1  post_position_2  weight_0  weight_1  weight_2  label_0  \\\n",
       "0              4.0              7.0     126.0     126.0     126.0    15.79   \n",
       "1              7.0              6.0     126.0     126.0     126.0    20.22   \n",
       "2              4.0              6.0     131.0     129.0     126.0    15.56   \n",
       "3              5.0              1.0     127.0     127.0     127.0    17.12   \n",
       "4              1.0              4.0     128.0     131.0     126.0    18.16   \n",
       "\n",
       "   label_1  label_2  \n",
       "0    18.04    17.69  \n",
       "1    17.50    20.40  \n",
       "2    12.04    15.35  \n",
       "3    15.86    17.50  \n",
       "4    15.92    18.76  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days_in_sequence = 3\n",
    "master_df = generate_master_df('./input_files/', days_in_sequence)\n",
    "master_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloader\n",
    "Create a generator that can parse through the master dataframe, and create batches of training data. These batches will have the shape (days_in_sequence, batch_size, input_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Section off data by race -- list of tuples (race_num, data)\n",
    "race_data = []\n",
    "for ii in range(days_in_sequence):\n",
    "    # Match all collumns for this race\n",
    "    pattern = re.compile('.*_{}'.format(ii))\n",
    "    cols = [pattern.match(col).string for col in master_df.columns if pattern.match(col) != None]\n",
    "    # Get data from these columns\n",
    "    data = master_df.loc[:, cols]\n",
    "    # Rename columns\n",
    "    cols = [col[:-2] for col in cols]\n",
    "    data.columns = cols\n",
    "    # Append to race data\n",
    "    race_data.append((ii, data)) \n",
    "    \n",
    "# Break race_data into input_data and label_data\n",
    "input_data = []\n",
    "labels = []\n",
    "for race_tup in race_data:\n",
    "    input_data.append(race_tup[1].drop(['label'], axis=1).values)\n",
    "    labels.append(race_tup[1]['label'].values)\n",
    "    \n",
    "# Want data to go in reverse order (oldest races first)\n",
    "input_data.reverse()\n",
    "labels.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(data, labels, days_in_sequence=3, batch_size=10, input_features=6):\n",
    "    # Truncate data to ensure only full batches\n",
    "    num_horses = len(data[0])\n",
    "    cutoff = (num_horses//batch_size)*batch_size\n",
    "    trunc_data = [race[:cutoff] for race in data]\n",
    "    trunc_labels = [race[:cutoff] for race in labels]\n",
    "    \n",
    "    # Create our batches\n",
    "    for ii in range(0, cutoff, batch_size):\n",
    "        # Get data for this batch\n",
    "        batch_data = [race[ii:ii+batch_size] for race in trunc_data]\n",
    "        batch_labels = [race[ii: ii+batch_size] for race in trunc_labels]\n",
    "        \n",
    "        # Create batch tensor of correct size -- days_in_sequence X batch_size X input_features\n",
    "        batch = torch.zeros((days_in_sequence, batch_size, input_features), dtype=torch.float64)\n",
    "        \n",
    "        # Fill in batch tensor\n",
    "        for batch_col in range(0, batch_size):\n",
    "            # Create sequence -- grab horse data from each race -- and add to batch\n",
    "            sequence = torch.tensor([batch_data[i][batch_col] for i in range(0, days_in_sequence)])\n",
    "            batch[:, batch_col] = sequence\n",
    "            \n",
    "        # Create label tensor\n",
    "        label_tensor = torch.tensor(batch_labels[-1], dtype=torch.float64)\n",
    "        \n",
    "        yield batch, label_tensor\n",
    "    \n",
    "    \n",
    "test = dataloader(input_data, labels)\n",
    "sample_batch, sample_label = next(iter(dataloader(input_data, labels, batch_size=64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandicappingBrain(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_length=6,lstm_size=64, lstm_layers=1, output_size=1, \n",
    "                               drop_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.input_length = input_length\n",
    "        self.output_size = output_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        ## LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_length, lstm_size, lstm_layers, \n",
    "                            dropout=drop_prob, batch_first=False)\n",
    "        \n",
    "        ## Dropout Layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## Fully-connected Output Layer\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "      \n",
    "    \n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        '''\n",
    "            Perform a forward pass through the network\n",
    "            \n",
    "            Args:\n",
    "                nn_input: the batch of input to NN\n",
    "                hidden_state: The LSTM hidden/cell state tuple\n",
    "                \n",
    "            Returns:\n",
    "                logps: log softmax output\n",
    "                hidden_state: the updated hidden/cell state tuple\n",
    "        '''\n",
    "        # Input -> LSTM\n",
    "        lstm_out, hidden_state = self.lstm(nn_input, hidden_state)\n",
    "\n",
    "        # Stack up LSTM outputs -- this gets the final LSTM output for each sequence in the batch\n",
    "        lstm_out = lstm_out[-1, :, :]\n",
    "        \n",
    "        # LSTM -> Dense Layer\n",
    "        dense_out = self.dropout(self.fc(lstm_out))\n",
    "        \n",
    "        # Return the final output and the hidden state\n",
    "        return dense_out, hidden_state\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "              weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\nn\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5832],\n",
      "        [0.0000],\n",
      "        [0.5118],\n",
      "        [0.6273],\n",
      "        [0.6036],\n",
      "        [0.0000],\n",
      "        [0.5869],\n",
      "        [0.6289],\n",
      "        [0.0000],\n",
      "        [0.5830]], dtype=torch.float64, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_model = HandicappingBrain(input_length=6, lstm_size=8, lstm_layers=1, drop_prob=0.2, output_size=1).double()\n",
    "hidden = test_model.init_hidden(10)\n",
    "dense_out, _ = test_model.forward(sample_batch, hidden)\n",
    "print(dense_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Test/Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18298 18298\n",
      "4574 4574\n"
     ]
    }
   ],
   "source": [
    "test_prop = 0.2\n",
    "test_end_idx = int(len(input_data[0]) * test_prop)\n",
    "\n",
    "# Create test set -- test_prob% of our total data set\n",
    "test_data = [race[:test_end_idx] for race in input_data]\n",
    "test_labels = [race[:test_end_idx] for race in labels]\n",
    "\n",
    "# Craete training set\n",
    "train_data = [race[test_end_idx:] for race in input_data]\n",
    "train_labels = [race[test_end_idx:] for race in labels]\n",
    "\n",
    "print(len(train_data[0]), len(train_labels[0]))\n",
    "print(len(test_data[0]), len(test_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandicappingBrain(\n",
      "  (lstm): LSTM(6, 1024, num_layers=2, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Train on GPU if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define model -- set dtype to double since our data requires it\n",
    "model = HandicappingBrain(input_length=6, lstm_size=1024, lstm_layers=2, output_size=1, drop_prob=0.3).double()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Epoch 1/50... Training Loss 1900.869990... Validation Loss: 1517.332975...\n",
      "Epoch 1/50... Training Loss 2599.786531... Validation Loss: 1131.532238...\n",
      "Starting Epoch 2\n",
      "Epoch 2/50... Training Loss 2407.544914... Validation Loss: 2225.220498...\n",
      "Epoch 2/50... Training Loss 2629.078080... Validation Loss: 1125.883956...\n",
      "Starting Epoch 3\n",
      "Epoch 3/50... Training Loss 1971.866769... Validation Loss: 1361.247828...\n",
      "Epoch 3/50... Training Loss 2486.243064... Validation Loss: 1141.060681...\n",
      "Starting Epoch 4\n",
      "Epoch 4/50... Training Loss 2091.042543... Validation Loss: 1751.389857...\n",
      "Epoch 4/50... Training Loss 2768.922189... Validation Loss: 1058.478190...\n",
      "Starting Epoch 5\n",
      "Epoch 5/50... Training Loss 2061.326151... Validation Loss: 1323.774590...\n",
      "Epoch 5/50... Training Loss 2300.097934... Validation Loss: 1028.437441...\n",
      "Starting Epoch 6\n",
      "Epoch 6/50... Training Loss 2485.634028... Validation Loss: 1315.476787...\n",
      "Epoch 6/50... Training Loss 2416.815996... Validation Loss: 1018.582896...\n",
      "Starting Epoch 7\n",
      "Epoch 7/50... Training Loss 2283.554549... Validation Loss: 1124.718460...\n",
      "Epoch 7/50... Training Loss 2508.479943... Validation Loss: 1080.003171...\n",
      "Starting Epoch 8\n",
      "Epoch 8/50... Training Loss 2344.777057... Validation Loss: 1076.887204...\n",
      "Epoch 8/50... Training Loss 2683.700734... Validation Loss: 1076.979152...\n",
      "Starting Epoch 9\n",
      "Epoch 9/50... Training Loss 2170.130724... Validation Loss: 1181.346388...\n",
      "Epoch 9/50... Training Loss 2831.217751... Validation Loss: 1089.311880...\n",
      "Starting Epoch 10\n",
      "Epoch 10/50... Training Loss 1961.963296... Validation Loss: 1328.480995...\n",
      "Epoch 10/50... Training Loss 2100.505387... Validation Loss: 1103.422173...\n",
      "Starting Epoch 11\n",
      "Epoch 11/50... Training Loss 2150.220151... Validation Loss: 1145.105939...\n",
      "Epoch 11/50... Training Loss 2753.507870... Validation Loss: 1177.772546...\n",
      "Starting Epoch 12\n",
      "Epoch 12/50... Training Loss 2336.171391... Validation Loss: 1128.969793...\n",
      "Epoch 12/50... Training Loss 2691.920566... Validation Loss: 1152.267772...\n",
      "Starting Epoch 13\n",
      "Epoch 13/50... Training Loss 2434.188300... Validation Loss: 1193.764277...\n",
      "Epoch 13/50... Training Loss 2188.008981... Validation Loss: 1094.868004...\n",
      "Starting Epoch 14\n",
      "Epoch 14/50... Training Loss 2011.398089... Validation Loss: 1710.859225...\n",
      "Epoch 14/50... Training Loss 3046.392886... Validation Loss: 1188.262228...\n",
      "Starting Epoch 15\n",
      "Epoch 15/50... Training Loss 2178.199156... Validation Loss: 1380.203579...\n",
      "Epoch 15/50... Training Loss 2374.722492... Validation Loss: 1190.022635...\n",
      "Starting Epoch 16\n",
      "Epoch 16/50... Training Loss 2473.137774... Validation Loss: 1428.592105...\n",
      "Epoch 16/50... Training Loss 2777.922225... Validation Loss: 1146.375116...\n",
      "Starting Epoch 17\n",
      "Epoch 17/50... Training Loss 2751.201103... Validation Loss: 1189.656555...\n",
      "Epoch 17/50... Training Loss 2692.115829... Validation Loss: 1154.946344...\n",
      "Starting Epoch 18\n",
      "Epoch 18/50... Training Loss 1992.201817... Validation Loss: 1114.257372...\n",
      "Epoch 18/50... Training Loss 2572.836181... Validation Loss: 1298.624503...\n",
      "Starting Epoch 19\n",
      "Epoch 19/50... Training Loss 2354.415130... Validation Loss: 1314.131752...\n",
      "Epoch 19/50... Training Loss 2195.876310... Validation Loss: 1166.616995...\n",
      "Starting Epoch 20\n",
      "Epoch 20/50... Training Loss 2114.444665... Validation Loss: 1247.623538...\n",
      "Epoch 20/50... Training Loss 2960.725231... Validation Loss: 1191.764618...\n",
      "Starting Epoch 21\n",
      "Epoch 21/50... Training Loss 2091.525056... Validation Loss: 1319.980290...\n",
      "Epoch 21/50... Training Loss 2665.388724... Validation Loss: 1173.864682...\n",
      "Starting Epoch 22\n",
      "Epoch 22/50... Training Loss 2461.682443... Validation Loss: 1334.985010...\n",
      "Epoch 22/50... Training Loss 2809.819611... Validation Loss: 1166.633529...\n",
      "Starting Epoch 23\n",
      "Epoch 23/50... Training Loss 2061.199391... Validation Loss: 1250.045738...\n",
      "Epoch 23/50... Training Loss 2263.099700... Validation Loss: 1212.492173...\n",
      "Starting Epoch 24\n",
      "Epoch 24/50... Training Loss 2302.377574... Validation Loss: 1321.046187...\n",
      "Epoch 24/50... Training Loss 2232.404589... Validation Loss: 1202.863401...\n",
      "Starting Epoch 25\n",
      "Epoch 25/50... Training Loss 2142.363393... Validation Loss: 1291.978840...\n",
      "Epoch 25/50... Training Loss 2992.377397... Validation Loss: 1214.076531...\n",
      "Starting Epoch 26\n",
      "Epoch 26/50... Training Loss 2303.699375... Validation Loss: 1324.871414...\n",
      "Epoch 26/50... Training Loss 2290.903797... Validation Loss: 1202.540697...\n",
      "Starting Epoch 27\n",
      "Epoch 27/50... Training Loss 2406.130206... Validation Loss: 1299.965061...\n",
      "Epoch 27/50... Training Loss 2464.663592... Validation Loss: 1196.764813...\n",
      "Starting Epoch 28\n",
      "Epoch 28/50... Training Loss 2378.342547... Validation Loss: 1277.537604...\n",
      "Epoch 28/50... Training Loss 2849.377909... Validation Loss: 1217.502878...\n",
      "Starting Epoch 29\n",
      "Epoch 29/50... Training Loss 2381.483845... Validation Loss: 1323.014447...\n",
      "Epoch 29/50... Training Loss 2819.668905... Validation Loss: 1227.534598...\n",
      "Starting Epoch 30\n",
      "Epoch 30/50... Training Loss 2226.296398... Validation Loss: 1348.052450...\n",
      "Epoch 30/50... Training Loss 2199.746732... Validation Loss: 1191.967143...\n",
      "Starting Epoch 31\n",
      "Epoch 31/50... Training Loss 2353.988962... Validation Loss: 1323.167501...\n",
      "Epoch 31/50... Training Loss 2639.767860... Validation Loss: 1313.952029...\n",
      "Starting Epoch 32\n",
      "Epoch 32/50... Training Loss 2226.776277... Validation Loss: 1337.471040...\n",
      "Epoch 32/50... Training Loss 2614.713152... Validation Loss: 1226.094503...\n",
      "Starting Epoch 33\n",
      "Epoch 33/50... Training Loss 2537.610616... Validation Loss: 1290.725884...\n",
      "Epoch 33/50... Training Loss 2414.776694... Validation Loss: 1237.380795...\n",
      "Starting Epoch 34\n",
      "Epoch 34/50... Training Loss 2767.210215... Validation Loss: 2418.976515...\n",
      "Epoch 34/50... Training Loss 2624.695401... Validation Loss: 1346.175065...\n",
      "Starting Epoch 35\n",
      "Epoch 35/50... Training Loss 2667.664641... Validation Loss: 1243.662123...\n",
      "Epoch 35/50... Training Loss 2684.786066... Validation Loss: 1274.496305...\n",
      "Starting Epoch 36\n",
      "Epoch 36/50... Training Loss 2173.740870... Validation Loss: 1341.948019...\n",
      "Epoch 36/50... Training Loss 2535.769896... Validation Loss: 1264.216814...\n",
      "Starting Epoch 37\n",
      "Epoch 37/50... Training Loss 2296.579946... Validation Loss: 1568.086065...\n",
      "Epoch 37/50... Training Loss 2655.824080... Validation Loss: 1267.627443...\n",
      "Starting Epoch 38\n",
      "Epoch 38/50... Training Loss 2286.845316... Validation Loss: 1505.032550...\n",
      "Epoch 38/50... Training Loss 2358.225755... Validation Loss: 1248.894156...\n",
      "Starting Epoch 39\n",
      "Epoch 39/50... Training Loss 2182.614963... Validation Loss: 1449.853717...\n",
      "Epoch 39/50... Training Loss 2562.664185... Validation Loss: 1248.140562...\n",
      "Starting Epoch 40\n",
      "Epoch 40/50... Training Loss 2459.038805... Validation Loss: 1314.853315...\n",
      "Epoch 40/50... Training Loss 2805.750477... Validation Loss: 1384.245025...\n",
      "Starting Epoch 41\n",
      "Epoch 41/50... Training Loss 2404.159074... Validation Loss: 1212.468119...\n",
      "Epoch 41/50... Training Loss 2397.404880... Validation Loss: 1293.269967...\n",
      "Starting Epoch 42\n",
      "Epoch 42/50... Training Loss 2249.789068... Validation Loss: 1460.055602...\n",
      "Epoch 42/50... Training Loss 2801.609398... Validation Loss: 1283.871760...\n",
      "Starting Epoch 43\n",
      "Epoch 43/50... Training Loss 2196.431730... Validation Loss: 1298.604895...\n",
      "Epoch 43/50... Training Loss 2667.493706... Validation Loss: 1401.424880...\n",
      "Starting Epoch 44\n",
      "Epoch 44/50... Training Loss 1932.500673... Validation Loss: 1290.604549...\n",
      "Epoch 44/50... Training Loss 2637.941665... Validation Loss: 1314.479223...\n",
      "Starting Epoch 45\n",
      "Epoch 45/50... Training Loss 2194.758972... Validation Loss: 1278.461968...\n",
      "Epoch 45/50... Training Loss 2339.306328... Validation Loss: 1301.768799...\n",
      "Starting Epoch 46\n",
      "Epoch 46/50... Training Loss 2363.025237... Validation Loss: 1562.465067...\n",
      "Epoch 46/50... Training Loss 2705.438848... Validation Loss: 1340.226470...\n",
      "Starting Epoch 47\n",
      "Epoch 47/50... Training Loss 2141.241876... Validation Loss: 1253.931037...\n",
      "Epoch 47/50... Training Loss 2462.862756... Validation Loss: 1319.921224...\n",
      "Starting Epoch 48\n",
      "Epoch 48/50... Training Loss 2404.011273... Validation Loss: 1225.534585...\n",
      "Epoch 48/50... Training Loss 2776.963389... Validation Loss: 1307.563023...\n",
      "Starting Epoch 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50... Training Loss 2009.086918... Validation Loss: 1197.430074...\n",
      "Epoch 49/50... Training Loss 2761.267256... Validation Loss: 1350.152250...\n",
      "Starting Epoch 50\n",
      "Epoch 50/50... Training Loss 2009.169337... Validation Loss: 1254.915016...\n",
      "Epoch 50/50... Training Loss 2809.142296... Validation Loss: 1324.048582...\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 256\n",
    "learning_rate = 0.002\n",
    "seq_length = days_in_sequence\n",
    "clip = 5\n",
    "input_length = 6\n",
    "\n",
    "print_every = 25\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "training_losses = [x for x in range(epochs)]\n",
    "validation_losses = [x for x in range(epochs)]\n",
    "\n",
    "# Set to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting Epoch {}'.format(epoch+1))\n",
    "    batches_processed = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    \n",
    "    # Get batch data\n",
    "    for batch, labels in dataloader(train_data, train_labels,\n",
    "                                                input_features=input_length,\n",
    "                                                days_in_sequence=seq_length,\n",
    "                                                batch_size=batch_size):\n",
    "        # Increment step count\n",
    "        batches_processed += 1\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        train_hidden = model.init_hidden(batch_size)\n",
    "        train_hidden = tuple([each.data for each in train_hidden])\n",
    "        \n",
    "        # Set tensors to correct device\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        for each in train_hidden:\n",
    "            each.to(device)\n",
    "            \n",
    "        # Zero out gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Run batch data through model\n",
    "        train_out, train_hidden = model(batch, train_hidden)\n",
    "        \n",
    "        # Calculate loss and perform back propogation -- clip gradients if necessary\n",
    "        loss = criterion(train_out, labels)\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Take optimizer step to update model weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation #\n",
    "        if batches_processed % print_every == 0:\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            \n",
    "            # Iterate through test data to validate model performance\n",
    "            for val_batch, val_labels in dataloader(test_data, test_labels,\n",
    "                                                    input_features=input_length,\n",
    "                                                    days_in_sequence=seq_length,\n",
    "                                                    batch_size=batch_size):\n",
    "                # Initialize hidden state\n",
    "                val_hidden = model.init_hidden(batch_size)\n",
    "                val_hidden = tuple([each.data for each in val_hidden])\n",
    "                \n",
    "                # Set tensors to correct device\n",
    "                val_batch, val_labels = val_batch.to(device), val_labels.to(device)\n",
    "                for each in val_hidden:\n",
    "                    each.to(device)\n",
    "                    \n",
    "                # Run data through network\n",
    "                val_output, val_hidden = model(val_batch, val_hidden)\n",
    "                \n",
    "                # Calculate loss\n",
    "                val_loss = criterion(val_output, val_labels)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            # Print out metrics\n",
    "            print('Epoch {}/{}...'.format(epoch+1, epochs),\n",
    "                  'Training Loss {:.6f}...'.format(loss.item()),\n",
    "                  'Validation Loss: {:.6f}...'.format(np.mean(val_losses)))\n",
    "            \n",
    "            # Record metrics\n",
    "            training_losses[epoch] = np.mean(train_losses)\n",
    "            validation_losses[epoch] = np.mean(val_losses)\n",
    "            \n",
    "            # Set model back to train\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Training/Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[56.3199],\n",
      "        [56.3195],\n",
      "        [56.3199],\n",
      "        [56.3203],\n",
      "        [56.3203],\n",
      "        [56.3201],\n",
      "        [56.3202],\n",
      "        [56.3203],\n",
      "        [56.3201],\n",
      "        [56.3196],\n",
      "        [56.3203],\n",
      "        [56.3203],\n",
      "        [56.3201],\n",
      "        [56.3203],\n",
      "        [56.3197],\n",
      "        [56.3203],\n",
      "        [56.3204],\n",
      "        [56.3188],\n",
      "        [56.3202],\n",
      "        [56.3203],\n",
      "        [56.3203],\n",
      "        [56.3200],\n",
      "        [56.3203],\n",
      "        [56.3206],\n",
      "        [56.3195],\n",
      "        [56.3203],\n",
      "        [56.3203],\n",
      "        [56.3204],\n",
      "        [56.3203],\n",
      "        [56.3163],\n",
      "        [56.3203],\n",
      "        [56.3203],\n",
      "        [56.3203],\n",
      "        [56.3199],\n",
      "        [56.3195],\n",
      "        [56.3199],\n",
      "        [56.3203],\n",
      "        [56.3203],\n",
      "        [56.3201],\n",
      "        [56.3202],\n",
      "        [56.3203],\n",
      "        [56.3201],\n",
      "        [56.3196],\n",
      "        [56.3203],\n",
      "        [56.3203],\n",
      "        [56.3201],\n",
      "        [56.3203],\n",
      "        [56.3197],\n",
      "        [56.3203],\n",
      "        [56.3204],\n",
      "        [56.3188],\n",
      "        [56.3202],\n",
      "        [56.3203],\n",
      "        [56.3203],\n",
      "        [56.3200],\n",
      "        [56.3203],\n",
      "        [56.3206],\n",
      "        [56.3195],\n",
      "        [56.3203],\n",
      "        [56.3203],\n",
      "        [56.3204],\n",
      "        [56.3203],\n",
      "        [56.3163],\n",
      "        [56.3203]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "hidden = model.init_hidden(64)\n",
    "sample_batch = sample_batch.to(device)\n",
    "for each in hidden:\n",
    "    each.to(device)\n",
    "dense_out, _ = model.forward(sample_batch, hidden)\n",
    "print(dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
