{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process DRF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_master_df(path, num_races=3):\n",
    "    '''\n",
    "        Generate the master dataframe from which we will create our training/testing data\n",
    "        \n",
    "        Args:\n",
    "            path (string): Path to directory containing DRF files to parse\n",
    "            num_races (int): Number of races to use in each sequence (how many races back\n",
    "                             are we looking?)\n",
    "        \n",
    "        Returns: Dataframe containing all data from each DRF concatted together\n",
    "    '''\n",
    "    # Cap num_races\n",
    "    num_races = min(num_races, 9) # Only have max of 9 prev race's data\n",
    "    \n",
    "    # Get all DRF files in data directory\n",
    "    filenames = [path+file for file in os.listdir(path) if file.endswith(\".DRF\")]\n",
    "    \n",
    "    # Iterate through each file and concat data to master df\n",
    "    master_df = None\n",
    "    for ii, file in tqdm(enumerate(filenames)): \n",
    "        if ii == 0:\n",
    "            # First pass through just create master df\n",
    "            df = pd.read_csv(file, header=None)\n",
    "            master_df = slice_df(df, num_races)\n",
    "        else:\n",
    "            # All other passes, append sliced dataframe to master\n",
    "            df = pd.read_csv(file, header=None)\n",
    "            df = slice_df(df, num_races)\n",
    "            master_df = master_df.append(df, ignore_index=True)\n",
    "            \n",
    "    # Drop all rows containing NaN values (these horses didn't have enough prev races)\n",
    "    master_df = master_df.dropna().reset_index().drop(['index'], axis=1)\n",
    "    # Derive additional fields from input data\n",
    "    master_df = derive_fields(master_df, num_races)\n",
    "    # Clean data and return\n",
    "    return clean_data(master_df, num_races)\n",
    "\n",
    "def slice_df(df, num_races=3):\n",
    "    '''\n",
    "        This will take the 1400 field dataframe created from parsing input file CSVs \n",
    "        and cut it down to only include information relevant to training the network\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): the unedited, 1400 field dataframe\n",
    "            num_races (int): the number of races into the past we are gathering data for\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame containing only fields relevant to network training\n",
    "    '''\n",
    "    # Define columns to grab\n",
    "    column_ids = OrderedDict({\n",
    "        'horse_age': (45,46),\n",
    "        'horse_name': (44,45),\n",
    "        'lifetime_starts': (96, 97),\n",
    "        'lifetime_wins': (97, 98),\n",
    "        'lifetime_places': (98, 99),\n",
    "        'lifetime_shows': (99, 100),\n",
    "        'days_since_prev_race': (265, 265+num_races),\n",
    "        'distance': (315, 315+num_races),\n",
    "        'num_entrants': (345, 345+num_races),\n",
    "        'post_position': (355, 355+num_races),\n",
    "        'weight': (505, 505+num_races),\n",
    "        'winner_name': (405, 405+num_races),\n",
    "        'place_name': (415, 415+num_races),\n",
    "        'show_name': (425, 425+num_races),\n",
    "        'label': (1035, 1036) # Finish time of current race\n",
    "    })\n",
    "\n",
    "    # Select all of our column ranges\n",
    "    rng = []\n",
    "    col_names = []\n",
    "    for k,v in column_ids.items():\n",
    "        # Append range to rng -- special case for single field\n",
    "        if v[1] - v[0] == 1:\n",
    "            for i in range(num_races):\n",
    "                rng += [v[0]]\n",
    "                col_names.append('{}_{}'.format(k, i))\n",
    "        else:\n",
    "            # Handle column ranges\n",
    "            rng += range(v[0],v[1])\n",
    "            for ii in range(v[0], v[1]):\n",
    "                col_names.append('{}_{}'.format(k, ii-v[0]))\n",
    "\n",
    "    # Slice df on columns\n",
    "    ret = df.loc[:, rng]\n",
    "    ret.columns = col_names\n",
    "    return ret\n",
    "\n",
    "def derive_fields(df, num_races=3):\n",
    "    '''\n",
    "        Derive fields such as lifetime wins/places/shows at each race in the past\n",
    "        and return dataframe containing derived fields, and not containing fields\n",
    "        required to do derivation.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing all fields necessary for derivations\n",
    "            num_races (int): the number of races into the past we are looking (includes today)\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame containing all data required to train network\n",
    "    '''\n",
    "    # Can skip Race 0 for all derived metrics since these are already calculated for us in the lifetime\n",
    "    # stats fields\n",
    "    drop_columns = []\n",
    "    for race in range(0, num_races):\n",
    "        # Calculate starts\n",
    "        start_col = 'lifetime_starts_{}'.format(race)\n",
    "        df[start_col] = df['lifetime_starts_0'] - race\n",
    "        \n",
    "        # Determine if a horse got win/place/show for previous races\n",
    "        win_col = 'winner_name_{}'.format(race)\n",
    "        place_col = 'place_name_{}'.format(race)\n",
    "        show_col = 'show_name_{}'.format(race)\n",
    "        horse_name_col = 'horse_name_{}'.format(race)\n",
    "        \n",
    "        # Can skip race 0, but still want to remove its intermediate cols\n",
    "        if race != 0:\n",
    "            horse_won_col = 'horse_won_{}'.format(race)\n",
    "            horse_placed_col = 'horse_placed_{}'.format(race)\n",
    "            horse_showed_col = 'horse_showed_{}'.format(race)\n",
    "\n",
    "            df[horse_won_col] = df[win_col] == df[horse_name_col]\n",
    "            df[horse_placed_col] = df[place_col] == df[horse_name_col]\n",
    "            df[horse_showed_col] = df[show_col] == df[horse_name_col]\n",
    "\n",
    "            # Calculate point-in-time lifetime stats using the above metrics\n",
    "            prev_lt_win_col = 'lifetime_wins_{}'.format(race-1)\n",
    "            lt_win_col = 'lifetime_wins_{}'.format(race)\n",
    "\n",
    "            prev_lt_place_col = 'lifetime_places_{}'.format(race-1)\n",
    "            lt_place_col = 'lifetime_places_{}'.format(race)\n",
    "\n",
    "            prev_lt_show_col = 'lifetime_shows_{}'.format(race-1)\n",
    "            lt_show_col = 'lifetime_shows_{}'.format(race)\n",
    "\n",
    "            # If horse won/placed/showed, decrement stats accordingly\n",
    "            df.loc[df[horse_won_col] == True, lt_win_col] = df[prev_lt_win_col] - 1\n",
    "            df.loc[df[horse_won_col] == False, lt_win_col] = df[prev_lt_win_col]\n",
    "\n",
    "            df.loc[df[horse_placed_col] == True, lt_place_col] = df[prev_lt_place_col] - 1\n",
    "            df.loc[df[horse_placed_col] == False, lt_place_col] = df[prev_lt_place_col]\n",
    "\n",
    "            df.loc[df[horse_showed_col] == True, lt_show_col] = df[prev_lt_show_col] - 1\n",
    "            df.loc[df[horse_showed_col] == False, lt_show_col] = df[prev_lt_show_col]\n",
    "        \n",
    "            # Append intermediate columns to drop list\n",
    "            drop_columns += [horse_won_col, horse_placed_col, horse_showed_col, win_col, place_col, show_col, horse_name_col]\n",
    "        else:\n",
    "            drop_columns += [win_col, place_col, show_col, horse_name_col]\n",
    "            \n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(drop_columns, axis=1)    \n",
    "        \n",
    "    return df\n",
    "\n",
    "def clean_data(df, num_races=3):\n",
    "    '''\n",
    "        Clean the data of any outliers or incorrect values\n",
    "    '''\n",
    "    # Drop horses that didn't finish one of their previous races (time == 0)\n",
    "    label_cols = ['label_{}'.format(race) for race in range(num_races)]\n",
    "    for col in label_cols:\n",
    "        df = df.loc[df[col] != 0]\n",
    "        \n",
    "    # Drop horses who have a negative lifetime stat (think these are a result of dqs)\n",
    "    for race in range(num_races):\n",
    "        show_col = 'lifetime_shows_{}'.format(race)\n",
    "        place_col = 'lifetime_places_{}'.format(race)\n",
    "        win_col = 'lifetime_wins_{}'.format(race)\n",
    "        \n",
    "        df = df.loc[df[show_col] >= 0]\n",
    "        df = df.loc[df[place_col] >= 0]\n",
    "        df = df.loc[df[win_col] >= 0]\n",
    "        \n",
    "    # Drop horses who have negative distance values for their races (bad data)\n",
    "    distance_cols = ['distance_{}'.format(race) for race in range(num_races)]\n",
    "    for col in distance_cols:\n",
    "        df = df.loc[df[col] >= 0]\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "419it [00:39,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lifetime_shows_0            0.00\n",
      "lifetime_shows_1            0.00\n",
      "lifetime_shows_2            0.00\n",
      "post_position_0             0.00\n",
      "post_position_1             0.00\n",
      "post_position_2             0.00\n",
      "num_entrants_0              3.00\n",
      "num_entrants_1              3.00\n",
      "num_entrants_2              3.00\n",
      "weight_0                  107.00\n",
      "weight_1                  106.00\n",
      "weight_2                  106.00\n",
      "lifetime_wins_0             0.00\n",
      "lifetime_wins_1             0.00\n",
      "lifetime_wins_2             0.00\n",
      "days_since_prev_race_0      1.00\n",
      "days_since_prev_race_1      1.00\n",
      "days_since_prev_race_2      3.00\n",
      "lifetime_places_0           0.00\n",
      "lifetime_places_1           0.00\n",
      "lifetime_places_2           0.00\n",
      "label_0                     6.57\n",
      "label_1                     6.57\n",
      "label_2                     6.57\n",
      "horse_age_0                 4.00\n",
      "horse_age_1                 4.00\n",
      "horse_age_2                 4.00\n",
      "distance_0                100.00\n",
      "distance_1                100.00\n",
      "distance_2                100.00\n",
      "lifetime_starts_0           4.00\n",
      "lifetime_starts_1           3.00\n",
      "lifetime_starts_2           2.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Pre-process input files to generate master dataframe\n",
    "#pd.set_option('display.max_columns', None) # Remove\n",
    "past_races_included = 3\n",
    "master_df = generate_master_df('./input_files/', num_races=past_races_included)\n",
    "master_df.head(5)\n",
    "\n",
    "print(master_df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22688\n"
     ]
    }
   ],
   "source": [
    "# Generate data and labels for dataset\n",
    "# Label column gets repeated as many times as there are races due to loading process\n",
    "label_cols = ['label_{}'.format(race) for race in range(past_races_included)]\n",
    "    \n",
    "master_data = master_df.drop(label_cols, axis=1)\n",
    "master_labels = master_df['label_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_dataloader(data, labels, batch_size=10):\n",
    "    # Only want full batches\n",
    "    num_full_batches = data.shape[0] // batch_size\n",
    "    total_entries = num_full_batches * batch_size\n",
    "    data = data[:total_entries]\n",
    "    labels = labels[:total_entries]\n",
    "    \n",
    "    for ii in range(0, total_entries, batch_size):\n",
    "        # Get only data and labels needed for this batch\n",
    "        batch_data = data[ii:ii+batch_size]\n",
    "        batch_labels = labels[ii:ii+batch_size]\n",
    "        \n",
    "        yield batch_data, batch_labels\n",
    "        \n",
    "def dataloader(data, labels):\n",
    "    for ii in range(len(data)):\n",
    "        # Turn data and label into tensor\n",
    "        yield torch.tensor(data.iloc[ii]), torch.tensor(labels.iloc[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.0000e+00, 4.0000e+00, 4.0000e+00, 8.0000e+00, 4.0000e+00, 3.0000e+00,\n",
      "        9.0000e+00, 1.1000e+01, 8.0000e+00, 1.5000e+02, 1.4400e+02, 1.5000e+02,\n",
      "        3.0000e+00, 3.0000e+00, 3.0000e+00, 4.3000e+01, 2.1000e+01, 1.4000e+01,\n",
      "        2.0000e+00, 2.0000e+00, 2.0000e+00, 7.0000e+00, 7.0000e+00, 7.0000e+00,\n",
      "        2.6400e+03, 3.9600e+03, 3.7400e+03, 2.9000e+01, 2.8000e+01, 2.7000e+01]) tensor(160.2200)\n"
     ]
    }
   ],
   "source": [
    "# Test dataloader\n",
    "sample_data, sample_label = next(iter(dataloader(master_data, master_labels)))\n",
    "print(sample_data, sample_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training/Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows where label is 0\n",
    "\n",
    "train_prop = 0.8 # Proportion of training data to testing data\n",
    "train_end_idx = int(len(master_data) * 0.8)\n",
    "\n",
    "train_data = master_data[:train_end_idx]\n",
    "train_labels = master_labels[:train_end_idx]\n",
    "\n",
    "test_data = master_data[train_end_idx:]\n",
    "test_labels = master_labels[train_end_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize Test and Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(lifetime_shows_0          1.922368e-15\n",
      "lifetime_shows_1          1.900482e-15\n",
      "lifetime_shows_2          8.314378e-16\n",
      "post_position_0           1.244979e-16\n",
      "post_position_1          -1.907014e-16\n",
      "post_position_2          -1.982742e-16\n",
      "num_entrants_0           -4.606818e-16\n",
      "num_entrants_1           -1.232904e-15\n",
      "num_entrants_2           -2.707534e-15\n",
      "weight_0                 -1.019692e-16\n",
      "weight_1                  2.030209e-17\n",
      "weight_2                 -1.561333e-15\n",
      "lifetime_wins_0          -1.022457e-15\n",
      "lifetime_wins_1           2.363472e-15\n",
      "lifetime_wins_2           4.749136e-15\n",
      "days_since_prev_race_0   -3.360887e-16\n",
      "days_since_prev_race_1   -1.403530e-17\n",
      "days_since_prev_race_2    3.791522e-17\n",
      "lifetime_places_0         4.346447e-16\n",
      "lifetime_places_1        -1.843423e-15\n",
      "lifetime_places_2         1.822087e-15\n",
      "horse_age_0               4.287455e-15\n",
      "horse_age_1               4.287455e-15\n",
      "horse_age_2               4.287455e-15\n",
      "distance_0                6.249298e-16\n",
      "distance_1               -4.900904e-15\n",
      "distance_2                1.202439e-15\n",
      "lifetime_starts_0        -1.262412e-15\n",
      "lifetime_starts_1        -1.262412e-15\n",
      "lifetime_starts_2        -1.262412e-15\n",
      "dtype: float64, lifetime_shows_0          0.999972\n",
      "lifetime_shows_1          0.999972\n",
      "lifetime_shows_2          0.999972\n",
      "post_position_0           0.999972\n",
      "post_position_1           0.999972\n",
      "post_position_2           0.999972\n",
      "num_entrants_0            0.999972\n",
      "num_entrants_1            0.999972\n",
      "num_entrants_2            0.999972\n",
      "weight_0                  0.999972\n",
      "weight_1                  0.999972\n",
      "weight_2                  0.999972\n",
      "lifetime_wins_0           0.999972\n",
      "lifetime_wins_1           0.999972\n",
      "lifetime_wins_2           0.999972\n",
      "days_since_prev_race_0    0.999972\n",
      "days_since_prev_race_1    0.999972\n",
      "days_since_prev_race_2    0.999972\n",
      "lifetime_places_0         0.999972\n",
      "lifetime_places_1         0.999972\n",
      "lifetime_places_2         0.999972\n",
      "horse_age_0               0.999972\n",
      "horse_age_1               0.999972\n",
      "horse_age_2               0.999972\n",
      "distance_0                0.999972\n",
      "distance_1                0.999972\n",
      "distance_2                0.999972\n",
      "lifetime_starts_0         0.999972\n",
      "lifetime_starts_1         0.999972\n",
      "lifetime_starts_2         0.999972\n",
      "dtype: float64) (lifetime_shows_0         -2.167602e-16\n",
      "lifetime_shows_1          8.785392e-17\n",
      "lifetime_shows_2         -3.965598e-16\n",
      "post_position_0          -6.996998e-18\n",
      "post_position_1          -7.494862e-17\n",
      "post_position_2          -8.017190e-17\n",
      "num_entrants_0            3.683547e-16\n",
      "num_entrants_1            2.416656e-16\n",
      "num_entrants_2           -2.201853e-16\n",
      "weight_0                  5.567996e-16\n",
      "weight_1                  4.430739e-16\n",
      "weight_2                 -7.105745e-16\n",
      "lifetime_wins_0           1.035544e-15\n",
      "lifetime_wins_1           1.578361e-16\n",
      "lifetime_wins_2           4.214101e-17\n",
      "days_since_prev_race_0    1.402335e-16\n",
      "days_since_prev_race_1   -5.025117e-17\n",
      "days_since_prev_race_2   -1.550471e-17\n",
      "lifetime_places_0         4.740099e-16\n",
      "lifetime_places_1         2.039160e-16\n",
      "lifetime_places_2         6.923603e-17\n",
      "horse_age_0              -1.183373e-15\n",
      "horse_age_1              -1.183373e-15\n",
      "horse_age_2              -1.183373e-15\n",
      "distance_0                6.587943e-16\n",
      "distance_1                1.998915e-15\n",
      "distance_2               -3.784031e-15\n",
      "lifetime_starts_0        -2.578614e-17\n",
      "lifetime_starts_1        -2.578614e-17\n",
      "lifetime_starts_2        -2.578614e-17\n",
      "dtype: float64, lifetime_shows_0          0.99989\n",
      "lifetime_shows_1          0.99989\n",
      "lifetime_shows_2          0.99989\n",
      "post_position_0           0.99989\n",
      "post_position_1           0.99989\n",
      "post_position_2           0.99989\n",
      "num_entrants_0            0.99989\n",
      "num_entrants_1            0.99989\n",
      "num_entrants_2            0.99989\n",
      "weight_0                  0.99989\n",
      "weight_1                  0.99989\n",
      "weight_2                  0.99989\n",
      "lifetime_wins_0           0.99989\n",
      "lifetime_wins_1           0.99989\n",
      "lifetime_wins_2           0.99989\n",
      "days_since_prev_race_0    0.99989\n",
      "days_since_prev_race_1    0.99989\n",
      "days_since_prev_race_2    0.99989\n",
      "lifetime_places_0         0.99989\n",
      "lifetime_places_1         0.99989\n",
      "lifetime_places_2         0.99989\n",
      "horse_age_0               0.99989\n",
      "horse_age_1               0.99989\n",
      "horse_age_2               0.99989\n",
      "distance_0                0.99989\n",
      "distance_1                0.99989\n",
      "distance_2                0.99989\n",
      "lifetime_starts_0         0.99989\n",
      "lifetime_starts_1         0.99989\n",
      "lifetime_starts_2         0.99989\n",
      "dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "# Will subtract mean of each column from each element in that column, then divide difference by std of column\n",
    "train_data = (train_data.subtract(train_data.mean()))/train_data.std()\n",
    "test_data = (test_data.subtract(test_data.mean()))/test_data.std()\n",
    "\n",
    "print((np.mean(train_data), np.std(train_data)), (np.mean(test_data), np.std(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=30, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.2)\n",
      "  (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.2)\n",
      "  (6): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_features = train_data.shape[1]\n",
    "output_size = 1\n",
    "drop_prob = 0.2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(drop_prob),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(drop_prob),\n",
    "    nn.Linear(256, output_size)\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nate/anaconda3/envs/julie-stav-ws/lib/python3.5/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/nate/anaconda3/envs/julie-stav-ws/lib/python3.5/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2... Entries Processed: 500 Training Loss: 1416.314964... Validation Loss: 330.915187...\n",
      "Epoch: 1/2... Entries Processed: 1000 Training Loss: 842.478808... Validation Loss: 372.504630...\n",
      "Epoch: 1/2... Entries Processed: 1500 Training Loss: 616.243961... Validation Loss: 329.914970...\n",
      "Epoch: 1/2... Entries Processed: 2000 Training Loss: 517.455395... Validation Loss: 307.274836...\n",
      "Epoch: 1/2... Entries Processed: 2500 Training Loss: 453.350680... Validation Loss: 329.357551...\n",
      "Epoch: 1/2... Entries Processed: 3000 Training Loss: 395.453992... Validation Loss: 390.240710...\n",
      "Epoch: 1/2... Entries Processed: 3500 Training Loss: 356.729074... Validation Loss: 379.116907...\n",
      "Epoch: 1/2... Entries Processed: 4000 Training Loss: 324.779290... Validation Loss: 346.515146...\n",
      "Epoch: 1/2... Entries Processed: 4500 Training Loss: 298.913446... Validation Loss: 339.747958...\n",
      "Epoch: 1/2... Entries Processed: 5000 Training Loss: 284.645911... Validation Loss: 316.347209...\n",
      "Epoch: 1/2... Entries Processed: 5500 Training Loss: 267.138483... Validation Loss: 310.487734...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-4988ecebdb30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mentries_processed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-f5123f229002>\u001b[0m in \u001b[0;36mdataloader\u001b[0;34m(data, labels)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Turn data and label into tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/julie-stav-ws/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/julie-stav-ws/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1830\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_valid_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1832\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/julie-stav-ws/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_loc\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use GPU if possible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Define Training/Validation Loop\n",
    "epochs = 2\n",
    "lr = 0.003\n",
    "print_every = 500\n",
    "clip = 5\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "training_losses = [x for x in range(epochs)]\n",
    "validation_losses = [x for x in range(epochs)]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting Epoch {}'.format(epoch+1))\n",
    "    entries_processed = 0\n",
    "    \n",
    "    # Begin Training Loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for data, label in dataloader(train_data, train_labels):\n",
    "        entries_processed += 1\n",
    "        \n",
    "        # Set tensors to correct device\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        \n",
    "        # Zero out gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Perform pass through network\n",
    "        train_out = model(data)\n",
    "        \n",
    "        # Calculate Loss and perform backprop -- clip gradients if necessary\n",
    "        train_loss = criterion(train_out, label)\n",
    "        train_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Log Loss\n",
    "        train_losses.append(train_loss.item())\n",
    "    \n",
    "        # Take optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        if entries_processed % print_every == 0:\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            for data, label in dataloader(test_data, test_labels):\n",
    "                # Set tensors to correct device\n",
    "                data, label = data.to(device), label.to(device)\n",
    "                \n",
    "                # Generate prediction\n",
    "                val_out = model(data)\n",
    "                \n",
    "                # Calculate and log loss\n",
    "                val_loss = criterion(val_out, label)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                # Set back to training mode\n",
    "                model.train()\n",
    "                \n",
    "            # Print Metrics\n",
    "            print(\n",
    "                'Epoch: {}/{}...'.format(epoch+1, epochs),\n",
    "                'Entries Processed: {}'.format(entries_processed),\n",
    "                'Training Loss: {:.6f}...'.format(np.mean(train_losses)),\n",
    "                'Validation Loss: {:.6f}...'.format(np.mean(val_losses))\n",
    "                 )\n",
    "            \n",
    "        # Log Epoch-level metrics\n",
    "        training_losses[epoch] = np.mean(train_losses)\n",
    "        validation_losses[epoch] = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training losses\n",
    "plt.title('Training Losses')\n",
    "plt.plot(training_losses)\n",
    "plt.show()\n",
    "\n",
    "# Plot Validation Losses\n",
    "plt.title('Validation Losses')\n",
    "plt.plot(validation_losses)\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:julie-stav-ws]",
   "language": "python",
   "name": "conda-env-julie-stav-ws-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
